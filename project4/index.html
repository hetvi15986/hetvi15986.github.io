<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 4</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 20px auto;
            text-align: left;
        }
        h1 {
            color: #530f0f;
            text-align: center;
            
        }
        h2 {
            color: #8e1600;
            text-align: center;
            margin-top: 20px;
        }
        h3 {
            color: #804141;
            margin: 20px 10px 10px 10px;
            text-align: center;
        }
        h4 {
            color: #664848;
            margin: 20px 10px 10px 10px;
        }
        h5 {
            color: #a26b6b;
            margin: 20px 10px 10px 10px;
        }
        p {
            margin: 20px 10px 10px 10px;
        }
        
        img {
            max-width: 600px;
            height: auto;
            margin: 20px 10px 10px 10px;

        }

        figure {
            text-align: center; /* centers both image and caption */
            margin: 20px 0;
        }
        figcaption {
            font-family: 'Open Sans', sans-serif;
            font-size: 14px;
            color: #555;
            margin-top: 5px;
        }
        .image-row {
            display: flex;         
            gap: 20px;             
            flex-wrap: wrap;       
            justify-content: center; 
        }
        .image-row figure {
            margin: 0;
            text-align: center;
            flex: 0 1 400px;       
        }
        .image-row img {
            max-width: 100%;
            height: auto;
            display: block;
        }
        .image-column {
            display: flex;
            flex-direction: column; 
            gap: 20px;      
            align-items: center;
        }

        .image-column figure {
            margin: 0;
            text-align: center;
        }

        .image-column img {
            max-width: 90%;  
            height: auto;
            display: block;
        }

    </style>
</head>

<body>
<h1>Project 4: Neural Radiance Field!</h1>

<h2> Part 0: Calibrating Your Camera and Capturing a 3D Scan </h2> <p> In this part, I calibrate my camera and prepare images to perform NeRF on later, in part 2.6 of my own object.</p>
                        <h3>0.1 Camera Calibration</h3> <p> Calculate the intrisics of my camera by print siz 4x4 ArUco tags and capturing 30-50 images with my phone, varying angles and distances for robust calibration. </p>
                        <h3>0.2 Capturing a 3D Object Scan</h3> <p> To capture my object, I placed it next to one ArUco tag and captured 30-50 images, using differing angles and consistent zoom, keeping the distance 10-20cm. </p>
                        <h3>0.3 Estimating Camera Pose</h3> <p> For each image, I detect the ArUco tag and solve the PnP problem with <code>cv2.solvePnP()</code>, inputting my camera's intrinsic matrix and distortion coefficients calculated in part 0.1. I get the world to camera (w2c) matrices, from which I can calculate the c2w matrices for each camera through matrix inversion.</p> <p> Deliverable: 2 screenshots of your cloud of cameras in Viser showing the camera frustums' poses and images. </p>

<div class="image-row"> 
<figure> <img src="part1/sunflower/sunflower.png">  </figure>
</figure> <figure> <img src="part1/sunflower/sunflower.png"> </figure> 
</div class="image-row"></div>

                        <h3>0.4 Undistorting images and creating a dataset </h3> <h4>Undistorting Images</h4> <p> I removed lens distortions with <code>cv2.undistort()</code>, inputting each image, the camera's matrix, and distortion coefficients. I did some cropping to the region of interest, to remove invalid pixels as a result of this correction.  In the end, I updated the principal points of the intrinsic matrix that will be used in the dataset for NeRF. </p> <h4>Saving the Dataset</h4> <p>Once all of the images are undistorted, I saved the images, their corresponding camera poses (extrinsics) in a .npzalong with camera poses in a <code>.npz</code> format for NeRF. Through intrisic matrix K isn't explicitly passed through, it can just be recalculated assuming fx ~ fy </p> <p> Datasets in this format can now be loaded in for Part 2, and used to train/validate/test NeRF for both the Lego truck and my own object.</p>





<h2> Part 1: Fit a Neural Field to a 2D Image </h2> <p> GOAL:  To represent the 2D neural field <code>F: (u,v) → (r,g,b)</code> </p>
        <h3>Model Architecture and Hyperparameters</h3>
                <p> To represent the 2D neural field  <code>F: (u,v) → (r,g,b)</code>, I implemented a multilayer perceptron (MLP) with sinusoidal positional encoding. The network takes normalized 2D pixel coordinates as input and predicts an RGB value in the range <code>[0, 1]</code>. </p> <p> I closely followed the architecture shown in the image below.  </p> <figure> <img src="part1/mlp_img-1.jpg"> </figure>
                <p><b>Positional Encoding.</b>   The input coordinate <code>(u, v)</code> is expanded using sinusoidal positional encoding with a maximum frequency level of <b>L = 10</b>. This produces a <b>42-dimensional</b> encoded vector: </p> <p style="margin-left:30px"> <code>[u, v, sin(2⁰πu), cos(2⁰πu), …, sin(2¹⁰πv), cos(2¹⁰πv)]</code> </p> <p> Keeping the original 2 input dimensions, plus <code>2 × L × 2 = 40</code> sinusoidal terms, gives a final input width of: <b>42 channels</b> after positional encoding. </p>

                <p><b>MLP Structure.</b>   The encoded 42-D vector is fed into a fully-connected network consisting of: </p> <ul style="margin-left:20px"> <li>Linear(42 → 256), ReLU</li> <li>Linear(256 → 256), ReLU</li> <li>Linear(256 → 256), ReLU</li> <li>Linear(256 → 3), Sigmoid</li> </ul> <p> This results in a 4-layer MLP with <b>256 hidden units</b> in each hidden layer.   The final sigmoid ensures that predicted RGB values lie in <code>[0, 1]</code>. </p>
                <p><b>Training Setup.</b></p> <ul style="margin-left:20px">  <li>Optimizer: Adam</li>  <li>Learning rate: 1e-2</li> <li>Batch size: 10,000 px samples per iteration</li> <li>Iterations: 1,000 </li> <li>Loss: Mean Squared Error (MSE)</li> <li>Metric: Peak Signal-to-Noise Ratio (PSNR)</li> </ul>

                <h4>Dataset Sampling</h4> <p> To train efficiently, we randomly sample a batch of pixels per iteration. Each batch contains the 2D coordinates and normalized RGB values for supervision. </p>
                <h4>Training Loop</h4>  <p> 1. Forward pass: predict RGB from coordinates<br> 2. Compute MSE loss between predicted and ground truth colors<br> 3. Backpropagate and update model weights with Adam<br> 4. Track PSNR metric and reconstructed images to track progress. </p>
        <h3> Fox Image </h3> 
                <h4> Original Image </h4> <figure> <img src="part1/fox/fox.jpg"> </figure>
                <h4> Training Progression Across Iterations </h4><div class="image-row">    <figure> <img src="part1/fox/progression.png"> </figure>    <figure> <img src="part1/fox/training_psnr.png"> </figure></div class="image-row"></div>
                <h4> Tune and Compare Hyperparameters</h4> <p> These are the final results for 2 choices of max positional encoding frequency (L = 10 and L = 2) and 2 choices of hidden channel width (256 and 64), displayed as a 2D grid. </p> <p> Their corresponding PSNR Graphs are also reported across iterations, on the same graph.  Keeping all other parameters the same, we can see the model does worse from our baseline (blue) when the max-freq paramater L is decreased significantly (see red and green lines), and worsened a little when only the hidden dimension is decreased (see orange line).</p> <div class="image-row"> <figure> <img src="part1/fox/compare_grid.png"> </figure> <figure> <img src="part1/fox/compare_psnr.png"> </figure> </div class="image-row"></div>

        <h3> Sunflower Image </h3>
                <h4> Original Image (from my backyard) </h4> <figure> <img src="part1/sunflower/sunflower.png"> </figure>
                <h4> Training Progression Across Iterations </h4> <div class="image-row"> <figure> <img src="part1/sunflower/progression.png"> </figure> <figure> <img src="part1/sunflower/training_psnr.png"> </figure> </div class="image-row"></div>
                <h4> Tune and Compare Hyperparameters</h4> <p> These are the final results for 2 choices of max positional encoding frequency (L = 10 and L = 2) and 2 choices of hidden channel width (256 and 64), displayed as a 2D grid. </p> <p> Their corresponding PSNR Graphs are also reported across iterations, on the same graph.  Again, keeping all other parameters the same, we can see the model does worse from our baseline (blue) when the max-freq paramater L is decreased significantly (see red and green lines), and worsened a little when only the hidden dimension is decreased (see orange line).</p> <div class="image-row"> <figure> <img src="part1/sunflower/compare_grid.png"> </figure> <figure> <img src="part1/sunflower/compare_psnr.png"> </figure> </div class="image-row"></div>





<h2> Part 2: Fit a Neural Radiance Field from Multi-view Images </h3>



















</body>




</html>


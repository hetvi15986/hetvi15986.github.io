<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 4</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 20px auto;
            text-align: left;
        }
        h1 {
            color: #530f0f;
            text-align: center;
            
        }
        h2 {
            color: #8e1600;
            text-align: center;
            margin-top: 20px;
        }
        h3 {
            color: #804141;
            margin: 20px 10px 10px 10px;
            text-align: center;
        }
        h4 {
            color: #664848;
            margin: 20px 10px 10px 10px;
        }
        h5 {
            color: #a26b6b;
            margin: 20px 10px 10px 10px;
        }
        p {
            margin: 20px 10px 10px 10px;
        }
        
        img {
            max-width: 600px;
            height: auto;
            margin: 20px 10px 10px 10px;

        }

        figure {
            text-align: center; /* centers both image and caption */
            margin: 20px 0;
        }
        figcaption {
            font-family: 'Open Sans', sans-serif;
            font-size: 14px;
            color: #555;
            margin-top: 5px;
        }
        .image-row {
            display: flex;         
            gap: 20px;             
            flex-wrap: wrap;       
            justify-content: center; 
        }
        .image-row figure {
            margin: 0;
            text-align: center;
            flex: 0 1 400px;       
        }
        .image-row img {
            max-width: 100%;
            height: auto;
            display: block;
        }
        .image-column {
            display: flex;
            flex-direction: column; 
            gap: 20px;      
            align-items: center;
        }

        .image-column figure {
            margin: 0;
            text-align: center;
        }

        .image-column img {
            max-width: 90%;  
            height: auto;
            display: block;
        }

    </style>
</head>




<body>
<h1>Project 4: Neural Radiance Field!</h1>

<h2> Part 0: Calibrating Your Camera and Capturing a 3D Scan </h2> <p> In this part, I calibrate my camera and prepare images to perform NeRF on later, in part 2.6 of my own object.</p>
                        <h3>0.1 Camera Calibration</h3> <p> Calculate the intrisics of my camera by print siz 4x4 ArUco tags and capturing 30-50 images with my phone, varying angles and distances for robust calibration. </p> <div class="image-row">  <figure> <img src="part1/cal1.png">  </figure> <figure> <img src="part1/cal2.png"> </figure>  </div class="image-row"></div>
                        <h3>0.2 Capturing a 3D Object Scan</h3> <p> To capture my object, I placed it next to one ArUco tag and captured 30-50 images, using differing angles and consistent zoom, keeping the distance 10-20cm. </p>
                        <h3>0.3 Estimating Camera Pose</h3> <p> For each image, I detect the ArUco tag and solve the PnP problem with <code>cv2.solvePnP()</code>, inputting my camera's intrinsic matrix and distortion coefficients calculated in part 0.1. I get the world to camera (w2c) matrices, from which I can calculate the c2w matrices for each camera through matrix inversion.</p> <p> Deliverable: 2 screenshots of your cloud of cameras in Viser showing the camera frustums' poses and images. </p>

<div class="image-row"> 
<figure> <img src="part1/cal22.png">  </figure>
<figure> <img src="part1/cal12.png"> </figure> 
</div class="image-row"></div>

                        <h3>0.4 Undistorting images and creating a dataset </h3> <h4>Undistorting Images</h4> <p> I removed lens distortions with <code>cv2.undistort()</code>, inputting each image, the camera's matrix, and distortion coefficients. I did some cropping to the region of interest, to remove invalid pixels as a result of this correction.  In the end, I updated the principal points of the intrinsic matrix that will be used in the dataset for NeRF. </p> <h4>Saving the Dataset</h4> <p>Once all of the images are undistorted, I saved the images, their corresponding camera poses (extrinsics) in a .npzalong with camera poses in a <code>.npz</code> format for NeRF. Through intrisic matrix K isn't explicitly passed through, it can just be recalculated assuming fx ~ fy </p> <p> Datasets in this format can now be loaded in for Part 2, and used to train/validate/test NeRF for both the Lego truck and my own object.</p>




<h2> Part 1: Fit a Neural Field to a 2D Image </h2> <p> GOAL:  To represent the 2D neural field <code>F: (u,v) → (r,g,b)</code> </p>
        <h3>Model Architecture and Hyperparameters</h3>
                <p> To represent the 2D neural field  <code>F: (u,v) → (r,g,b)</code>, I implemented a multilayer perceptron (MLP) with sinusoidal positional encoding. The network takes normalized 2D pixel coordinates as input and predicts an RGB value in the range <code>[0, 1]</code>. </p> <p> I closely followed the architecture shown in the image below.  </p> <figure> <img src="part1/mlp_img-1.jpg"> </figure>
                <p><b>Positional Encoding.</b>   The input coordinate <code>(u, v)</code> is expanded using sinusoidal positional encoding with a maximum frequency level of <b>L = 10</b>. This produces a <b>42-dimensional</b> encoded vector: </p> <p style="margin-left:30px"> <code>[u, v, sin(2⁰πu), cos(2⁰πu), …, sin(2¹⁰πv), cos(2¹⁰πv)]</code> </p> <p> Keeping the original 2 input dimensions, plus <code>2 × L × 2 = 40</code> sinusoidal terms, gives a final input width of: <b>42 channels</b> after positional encoding. </p>

                <p><b>MLP Structure.</b>   The encoded 42-D vector is fed into a fully-connected network consisting of: </p> <ul style="margin-left:20px"> <li>Linear(42 → 256), ReLU</li> <li>Linear(256 → 256), ReLU</li> <li>Linear(256 → 256), ReLU</li> <li>Linear(256 → 3), Sigmoid</li> </ul> <p> This results in a 4-layer MLP with <b>256 hidden units</b> in each hidden layer.   The final sigmoid ensures that predicted RGB values lie in <code>[0, 1]</code>. </p>
                <p><b>Training Setup.</b></p> <ul style="margin-left:20px">  <li>Optimizer: Adam</li>  <li>Learning rate: 1e-2</li> <li>Batch size: 10,000 px samples per iteration</li> <li>Iterations: 1,000 </li> <li>Loss: Mean Squared Error (MSE)</li> <li>Metric: Peak Signal-to-Noise Ratio (PSNR)</li> </ul>

                <h4>Dataset Sampling</h4> <p> To train efficiently, we randomly sample a batch of pixels per iteration. Each batch contains the 2D coordinates and normalized RGB values for supervision. </p>
                <h4>Training Loop</h4>  <p> 1. Forward pass: predict RGB from coordinates<br> 2. Compute MSE loss between predicted and ground truth colors<br> 3. Backpropagate and update model weights with Adam<br> 4. Track PSNR metric and reconstructed images to track progress. </p>
        <h3> Fox Image </h3> 
                <h4> Original Image </h4> <figure> <img src="part1/fox/fox.jpg"> </figure>
                <h4> Training Progression Across Iterations </h4><div class="image-row">    <figure> <img src="part1/fox/progression.png"> </figure>    <figure> <img src="part1/fox/training_psnr.png"> </figure></div class="image-row"></div>
                <h4> Tune and Compare Hyperparameters</h4> <p> These are the final results for 2 choices of max positional encoding frequency (L = 10 and L = 2) and 2 choices of hidden channel width (256 and 64), displayed as a 2D grid. </p> <p> Their corresponding PSNR Graphs are also reported across iterations, on the same graph.  Keeping all other parameters the same, we can see the model does worse from our baseline (blue) when the max-freq paramater L is decreased significantly (see red and green lines), and worsened a little when only the hidden dimension is decreased (see orange line).</p> <div class="image-row"> <figure> <img src="part1/fox/compare_grid.png"> </figure> <figure> <img src="part1/fox/compare_psnr.png"> </figure> </div class="image-row"></div>

        <h3> Sunflower Image </h3>
                <h4> Original Image (from my backyard) </h4> <figure> <img src="part1/sunflower/sunflower.png"> </figure>
                <h4> Training Progression Across Iterations </h4> <div class="image-row"> <figure> <img src="part1/sunflower/progression.png"> </figure> <figure> <img src="part1/sunflower/training_psnr.png"> </figure> </div class="image-row"></div>
                <h4> Tune and Compare Hyperparameters</h4> <p> These are the final results for 2 choices of max positional encoding frequency (L = 10 and L = 2) and 2 choices of hidden channel width (256 and 64), displayed as a 2D grid. </p> <p> Their corresponding PSNR Graphs are also reported across iterations, on the same graph.  Again, keeping all other parameters the same, we can see the model does worse from our baseline (blue) when the max-freq paramater L is decreased significantly (see red and green lines), and worsened a little when only the hidden dimension is decreased (see orange line).</p> <div class="image-row"> <figure> <img src="part1/sunflower/compare_grid.png"> </figure> <figure> <img src="part1/sunflower/compare_psnr.png"> </figure> </div class="image-row"></div>





<h2> Part 2: Fit a Neural Radiance Field from Multi-view Images </h3>

<h3>Part 2.1: Create Rays from Cameras</h3>
                    <h4> Camera to World </h4> <p> I need ways to go between the world, camera, and image ('pixel') coordinates in order to implement NeRF. </p> <p> The world to camera transform can be defined using a 3x3 rotation matrix R and a 3-dim translation vector t, formatted to create the 4x4 w2c matrix.  The camera to world c2w matrix is just the inverse of this matrix. So, we can find c2w using the provided w2c. </p> <p> Implementing a function <code>x_w = transform(c2w, x_c)</code> to go from camera to world required first making the 3D x_c coord into a 4D homogenous coordinate, to support matrix multiplication with a 4x4 c2w matrix. This matrix multiplication was done with batch matrix multiply in PyTorch, to support transformation of several batches of samples in training. Then, the coordinates are normalized and then unhomogenized back to be 3D. </p> <p> My implementation passed the <code>x == transform(c2w.inv(), transform(c2w, x)) </code> check </p>
                    <h4> Pixel to Camera </h4> <p> Given an intrisic matrix K, pixel location uv, and depth s, I implemented a function that calculates the camera coordinates, such that <code> x_c = pixel_to_camera(K, uv, s) </code>. This is done by matrix multiplying the inverse of K with the homogenous coordinates of uv, resulting in the three-dimensional camera coordinates. This function also supports batched coordinates. </p>
                    <h4> Pixel to Ray </h4> <p> Lastly, I implemented <code> ray_o, ray_d = pixel_to_ray(K, c2w, uv)</code>. Each ray is defined by its origin <code>ray_o</code> and its direction <code>ray_d</code>. Here, our origin is the camera location in world coordinates (<code>tvec</code>), which is how much the camera is shifted from the world origin we chose. The ray's direction is calculating a point along the depth  <code>s = 1</code> surface, and converting it to world coords, and normalizing. </p> <p> In code, I first take the input 2D pixel location and using the intrinsivs, find its 3D direction inside the camera. Then, I go from this camera space to the world space using the <code> transform </code> function. The ray's origin is just the translational vector in c2w, and the ray's direction is found by subtracting the point's camera coordinate by the origin, and normalizing. </p>



<h3>Part 2.2: Sampling</h3> <p> In Part 1, we sampled pixels from one image to get colors and coordinates. When working with several images in NeRF, we have several cameras (different extrinsic matrices) to work with as well. With these, we want to convert pixel coordinates into rays in 3D space, so that we can sample from them. After obtaining the rays, we want to be able to sample points from these rays, so we can pass them into our network to ensure it has a diverse dataset to train on. </p> 
                    <h4>Sampling Rays from Images</h4> <p> To sample images from rays, I use the <strong>RaysData</strong> class to be consistent and organized. It tracks the pixel coordinates, camera instrisics, c2w matrices, and flattened rays of each image, which is then used for sampling. When sampling N rays from the image, using <code> RaysData_dataset.sample_rays(N) </code>, I get every pixel location and its corresponding ray origin and direcction. This also supports batches, as are later used in training. </p>
                    <h4>Sampling Points Along Rays </h4> <p> To discretize rays into 3D points, I uniformaly create points in each ray's <code> [near, far]</code> range, adding small random perturbation during training to avoid overfitting. </p><p>This ensures that every iteration of training shows the network a diverse collection of points from the data, allowing it to best learn the scene more robustly.</p>


<h3>Part 2.3: Putting the Dataloading All Together</h3>  <p> To verify implementations have thus far been correct, I visualized the Lego dataset's cameras, rays, and sampled pixel points in 3D.</p>
                    <div class="image-row"> 
                        <figure> <img src="part2.3/cams.png"> </figure> <caption> Cameras </caption>
                        <figure> <img src="part2.3/camsrays.png"> </figure> <caption> Cameras and Rays </caption>
                        <figure> <img src= "part2.3/camsrayssamps.png"></figure> <caption> Cameras, Rays, and Sampled Points </caption>
                    </div class="image-row"></div>
                    <div class="image-row"> 
                        <figure> <img src="part2.3/val_im.png"> </figure> <caption> Original Image </caption>
                        <figure> <img src= "part2.3/topleft.png"></figure> <caption> Rays from only the Top Left Corner of the Image</caption>
                    </div class="image-row"></div>


<h3>Part 2.4: Neural Radiance Field</h3>  <p> Now, I implemented a NeRF MLP that takes in 3D world coordinates and view directions (both positionally encodeded), and predicts the <strong>density</strong> and <strong>color</strong> of each 3D point. </p>
            <h4> Network Structure and Details </h4>
                            <ul>
                                <li>Closely followed this image. </li> <figure> <img src="final2.5/mlp_nerf.png"> </figure>
                                <li>Initial layers process the positional encoding of 3D coordinates.</li>
                                <li>A <strong>skip connection</strong> injects the original positional encoding into the middle of the network, so as to remind the network of the original input.  This also helps allow making the MLP deeper. </li>
                                <li> The network splits into two branches after 8 fully connected layers, since density should not be affected by the ray direction.
                                    <ul>
                                        <li><strong>Density branch:</strong> Outputs positive density values using ReLU.</li>
                                        <li><strong>Color branch:</strong> Concatenates intermediate features with the encoded ray direction, uses a small MLP, then predicts RGB values using Sigmoid (range 0–1).</li>
                                    </ul>
                                </li>
                                <li> When positionally encoding, I used a higher max frequency of 10 for coordinates to capture finer details, and a lower frequency oof 4 on viewing direction. </li>
                            </ul>



<h3>Part 2.5: Volume Rendering </h3> <p> Volume rendering can be done through an approximation, where the final color of the ray is the sum of the color, transmittance, and probability of a ray terminating at each sample i. First, I calculated the alpha values through 1 - exp(-sigmas * step_size). Then, I calculated the total transmittance along that ray with <code>torch.cumprod</code>, as well as the weights for each sample as transimttance * alpha. Finally, I computed the final pixel color as the weighted sum of the weights and the rgbs. This implementation is checked by the given code snipper, since <code> assert torch.allclose(rendered_colors, correct, rtol=1e-4, atol=1e-4) </code> passed. </p>

<h3> Lego Dataset </h4> <p> To put everything together, I implemented a training loop that would render intermediate 10 validation images and record their PSNR values, as well as training loss/PSNR. Here are the results for the Lego Data set. </p>


<h4> Progression Across Iteration and Loss and PSNR Graphs </h4>
                    <div class="image-col"> 
                        <figure> <img src="final2.5/graphs.png"> </figure> <caption> Loss and PSNR Graphs </caption>
                        <figure> <img src="final2.5/val_ims.png"> </figure> <caption> Progression Across Iteration </caption>
                    </div class="image-col"></div>

<h4> Final Training GIF Rendering </h4> <figure> <img src="final2.5/lego_spherical.gif"> </figure> 
<h4> Parameters </h4> <p> learning_rate=5e-4, iters=2000, batch_size=10000, render_every=500, n_samples = 64, near = 2.0, far = 6.0   </p>






















</body>




</html>


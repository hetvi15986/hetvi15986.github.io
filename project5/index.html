<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 5 – Wicked (Glinda Edition)</title>
    <style>
        :root {
            --glinda-pink-soft: #f8c7e6;
            --glinda-pink-main: #f5a6df;
            --glinda-pink-deep: #d96fb9;
            --elphaba-green: #00b140;
            --emerald-deep: #02130c;
            --night-sky: #050608;
            --wicked-gray: #dbe3e0;
            --card-bg: #151821;
        }

        * { box-sizing: border-box; }

        body {
            margin: 0;
            font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
            background:
                radial-gradient(circle at top, rgba(245,166,223,0.18) 0, transparent 45%),
                radial-gradient(circle at 80% 10%, rgba(0,177,64,0.16) 0, transparent 50%),
                linear-gradient(to bottom, #050608, #02040a 55%, #000000);
            color: var(--wicked-gray);
            line-height: 1.7;
        }

        a {
            color: var(--glinda-pink-main);
            text-decoration: none;
        }
        a:hover { text-decoration: underline; }

        .page {
            max-width: 1100px;
            margin: 0 auto;
            padding: 32px 20px 64px;
        }

        header {
            position: sticky;
            top: 0;
            z-index: 50;
            backdrop-filter: blur(14px);
            background:
                linear-gradient(to right,
                    rgba(5,6,8,0.96),
                    rgba(5,6,8,0.9),
                    rgba(5,6,8,0.85));
            border-bottom: 1px solid rgba(245,166,223,0.6);
            box-shadow: 0 10px 30px rgba(0,0,0,0.65);
        }

        .header-inner {
            max-width: 1100px;
            margin: 0 auto;
            padding: 10px 20px;
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 16px;
        }

        .logo-lockup {
            display: flex;
            align-items: center;
            gap: 12px;
        }

        .logo-circle {
            width: 40px;
            height: 40px;
            border-radius: 999px;
            border: 2px solid rgba(245,198,230,0.9);
            display: flex;
            align-items: center;
            justify-content: center;
            background:
                radial-gradient(circle at 30% 0, #ffffff 0, #f8c7e6 32%, #f5a6df 58%, #d96fb9 100%);
            box-shadow:
                0 0 18px rgba(245,166,223,0.7),
                0 0 32px rgba(0,177,64,0.45);
            font-size: 22px;
            font-weight: 700;
            color: #4b2740;
        }

        .logo-text-main {
            font-weight: 700;
            letter-spacing: 0.16em;
            text-transform: uppercase;
            font-size: 11px;
            color: var(--glinda-pink-soft);
        }

        .logo-text-sub {
            font-size: 11px;
            color: #a5afac;
        }

        .nav-links {
            display: flex;
            gap: 10px;
            font-size: 11px;
            text-transform: uppercase;
            letter-spacing: 0.18em;
        }

        .nav-links a {
            padding: 5px 10px;
            border-radius: 999px;
            border: 1px solid rgba(245,166,223,0.7);
            background: radial-gradient(circle at 20% 0, rgba(255,255,255,0.28), rgba(245,166,223,0.16));
            color: #fff;
            box-shadow: 0 0 12px rgba(245,166,223,0.4);
            transition: all 0.18s ease-out;
        }
        .nav-links a:hover {
            background: radial-gradient(circle at 20% 0, rgba(255,255,255,0.45), rgba(245,166,223,0.32));
            transform: translateY(-1px);
        }

        .section {
            margin-top: 32px;
            padding: 24px 24px 28px;
            border-radius: 22px;
            background:
                radial-gradient(circle at top left, rgba(245,166,223,0.2), transparent 55%),
                radial-gradient(circle at bottom right, rgba(0,177,64,0.17), transparent 60%),
                var(--card-bg);
            border: 1px solid rgba(245,198,230,0.4);
            box-shadow:
                0 26px 70px rgba(0,0,0,0.85),
                0 0 24px rgba(245,166,223,0.18);
        }

        .section + .section { margin-top: 28px; }

        h1, h2, h3, h4, h5 {
            font-family: "Cinzel", "Times New Roman", serif;
            font-weight: 600;
            letter-spacing: 0.08em;
            text-transform: uppercase;
            margin: 0;
        }

        h1 {
            font-size: 26px;
            text-align: center;
            color: var(--glinda-pink-soft);
            text-shadow:
                0 0 24px rgba(245,166,223,0.9),
                0 0 36px rgba(0,177,64,0.6);
            margin-bottom: 14px;
        }

        h2 {
            font-size: 18px;
            color: var(--glinda-pink-main);
            margin-top: 18px;
            margin-bottom: 10px;
            border-bottom: 1px solid rgba(245,166,223,0.55);
            padding-bottom: 4px;
            text-align: left;
        }

        h3 {
            font-size: 15px;
            color: var(--elphaba-green);
            margin-top: 18px;
            margin-bottom: 6px;
            text-align: left;
        }

        h4 {
            font-size: 13px;
            color: #fdf3fb;
            margin-top: 14px;
            margin-bottom: 4px;
        }

        h5 {
            font-size: 12px;
            color: #c0ccd0;
            margin-top: 12px;
            margin-bottom: 4px;
        }

        p {
            margin: 8px 0;
            font-size: 14px;
            color: #e3ece9;
        }

        code {
            font-family: "JetBrains Mono", ui-monospace, Menlo, Monaco, Consolas, "Courier New", monospace;
            font-size: 12px;
            padding: 1px 4px;
            border-radius: 4px;
            background: rgba(2, 9, 11, 0.96);
            color: #b9ffcf;
        }

        pre {
            background: #05080a;
            border-radius: 10px;
            padding: 10px 12px;
            overflow-x: auto;
            border: 1px solid rgba(245,166,223,0.35);
        }
        pre code { padding: 0; background: transparent; }

        ul, ol { margin: 6px 0 8px 20px; }
        li { margin: 2px 0 4px; }

        figure {
            margin: 16px auto;
            text-align: center;
        }

        figcaption {
            font-size: 12px;
            color: #c3ced0;
            margin-top: 6px;
        }

        img {
            max-width: 100%;
            height: auto;
            border-radius: 14px;
            border: 1px solid rgba(219,227,224,0.2);
            box-shadow:
                0 18px 38px rgba(0, 0, 0, 0.85),
                0 0 18px rgba(0,177,64,0.4);
        }

        .image-row {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 16px;
            margin: 14px 0;
        }

        .image-row figure {
            flex: 0 1 180px;
            margin: 0;
        }

        .image-row img { width: 100%; image-rendering: auto; }

        .image-column {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 18px;
            margin: 14px 0;
        }

        .image-column figure {
            width: 100%;
            max-width: 820px;
        }

        figure.side-by-side {
            display: flex;
            align-items: center;
            gap: 16px;
            justify-content: center;
            flex-wrap: wrap;
        }

        figure.side-by-side img { max-width: 380px; }

        .section-label {
            font-size: 10px;
            text-transform: uppercase;
            letter-spacing: 0.22em;
            color: var(--glinda-pink-soft);
            text-align: right;
            margin-bottom: 4px;
        }

        .section-label .pill {
            display: inline-flex;
            align-items: center;
            gap: 6px;
            padding: 3px 9px;
            border-radius: 999px;
            border: 1px solid rgba(245,166,223,0.7);
            background: radial-gradient(circle at 20% 0, rgba(255,255,255,0.3), rgba(245,166,223,0.24));
        }

        .section-label .pill span.dot {
            width: 6px;
            height: 6px;
            border-radius: 999px;
            background: var(--elphaba-green);
            box-shadow: 0 0 8px rgba(0,177,64,0.8);
        }

        footer {
            max-width: 1100px;
            margin: 32px auto 40px;
            padding: 0 20px;
            font-size: 11px;
            text-align: center;
            color: #a7b1af;
        }

        .wicked-highlight {
            color: var(--glinda-pink-main);
            font-weight: 600;
        }

        @media (max-width: 720px) {
            .section { padding: 18px 14px 22px; }
            .nav-links { display: none; }
            h1 { font-size: 22px; }
            h2 { font-size: 16px; }
            .image-row figure { flex: 0 1 46%; }
        }
    </style>
</head>
<body>
<header>
    <div class="header-inner">
        <div class="logo-lockup">
            <div class="logo-circle">G</div>
            <div>
                <div class="logo-text-main">Wicked · Glinda & Elphaba</div>
                <div class="logo-text-sub">Project 5 · CS 180 · Fall 2024</div>
            </div>
        </div>
        <nav class="nav-links">
            <a href="#proj5a">Project 5A</a>
            <a href="#proj5b">Project 5B</a>
        </nav>
    </div>
</header>

<div class="page">
        <!-- Wicked title + Table of Contents -->
        <!-- Wicked title + Table of Contents with working anchor links -->
    <section class="section">
        <h1>
            <a href="https://cal-cs180.github.io/fa25/hw/proj5/index.html"
               style="color: var(--glinda-pink-soft); text-decoration: underline;">
                Fun With Diffusion Models
            </a>
        </h1>

        <h2>Table of Contents</h2>

        <h3><a href="#proj5a">Project 5A: The Power of Diffusion Models</a></h3>

                                    <h4><a href="#part0-setup">Part 0: Set Up</a></h4> <p> Click your heels all you want; without embeddings, you’re still lost in Kansas. </p>

                                     <h4><a href="#sec-1-1-forward">1.1 Implementing the Forward Process</a></h4> <p> Each timestep dissolves into pure noise, as if it just had a bad encounter with a bucket of water.</p>

                                    <h4><a href="#sec-1-2-classical">1.2 Classical Denoising</a></h4> <p>Gaussian blur is the Wizard behind the curtain– impressive at first, until you realize it's just smoke and mirrors.</p>

                                    <h4><a href="#sec-1-3-onestep">1.3 One-Step Denoising</a></h4> <p>The Glinda Approach: Toss toss. </p>

                                      <h4><a href="#sec-1-4-iterative">1.4 Iterative Denoising</a></h4> <p> Oz wasn't built in a day: iterative denoising trades randomness for structure one careful step at a time. </p>

                                        <h4><a href="#sec-1-5-sampling">1.5 Diffusion Model Sampling</a></h4> <p>Every sample is a leap of faith that Oz actually exists.</p>

                                        <h4><a href="#sec-1-6-cfg">1.6 Classifier-Free Guidance (CFG)</a></h4>  <p>Think of it as two witches arguing – the guidance scale decides whose magic wins.</p>

                                         <h4><a href="#sec-1-7-sdedit">1.7 Image-to-image Translation</a></h4> <p>“Defying Gravity” with SDedit: jump in mid‑noise.</p>
                                          <h4><a href="#sec-1-7-1-editing">1.7.1 Editing Hand-Drawn and Web Images</a></h4> <p>Making doodles Broadway-ready.</p>

                                         <h4><a href="#sec-1-7-2-inpainting">1.7.2 Inpainting</a></h4><p> Grant some guts, restore some brains, mend a heart — the model only touches what needs magic.</p>

                                            <h4><a href="#sec-1-7-3-text">1.7.3 Text-Conditional Image-to-image Translation</a></h4> <p>Even the Wizard listens when you spell it out carefully.</p>

                                            <h4><a href="#sec-1-8-anagrams">1.8 Visual Anagrams</a></h4> <p> Elphaba. Glinda. Elphaba. Glinda. I'm a witch. and I'm a witch. But which is witch? Well here's the sitch. </p>

                                                <h4><a href="#sec-1-9-hybrids">1.9 Hybrid Images</a></h4> <p>Every story has two sides.</p>

        <h3><a href="#proj5b">Project 5B: Flow Matching from Scratch</a></h3>

                                     <h4><a href="#b-part1-single-step">Part 1: Training a Single-Step Denoising UNet</a></h4><p>One‑step “There’s no place like home” click, straight back to Kansas.</p>

                                 <h4><a href="#b-1-1-unet">1.1 Implementing the UNet</a></h4>  <p> Down the tornado, up into full-color Oz.</p>

        <h4><a href="#b-1-2-train-denoiser">1.2 Using the UNet to Train a Denoiser</a></h4>
                                         <h4><a href="#b-1-2-1-training">1.2.1 Training</a></h4> <p> Glinda and Elphaba preparing to quell the chaos. </p>

                                          <h4><a href="#b-1-2-2-ood">1.2.2 Out-of-Distribution Testing</a></h4><p> Glinda hasn't practiced this spell yet.</p>

                                            <h4><a href="#b-1-2-3-pure-noise">1.2.3 Denoising Pure Noise</a></h4><p>Why the wizard of Oz couldn't get anything done.</p>

                                <h4><a href="#b-part2-flow">Part 2: Training a Flow Matching Model</a></h4><p>Flow matching is following the Yellow Brick Road from noisy chaos to clean digits; Dorothy had detours, but step by step she gets closer to the Emerald City.</p>

                                <h4><a href="#b-2-1-time-cond">2.1 Adding Time Conditioning to UNet</a></h4>  <p>Time conditioning is like knowing how far into the movie we are– early storm or final curtain in Emerald City.</p>

                                <h4><a href="#b-2-2-train-time">2.2 Training the UNet</a></h4> <p>Learning the winds of Oz.</p>

                                <h4><a href="#b-2-3-sample-time">2.3 Sampling from the UNet</a></h4><p>Follow the yellow brick road.</p>

                             <h4><a href="#b-2-4-class-cond">2.4 Adding Class-Conditioning to UNet</a></h4> <p>Class conditioning helps choose the destination: Munchkinland, Emerald City, or the Witch’s castle.</p>

                                      <h4><a href="#b-2-5-train-class">2.5 Training the UNet (Class-Conditional)</a></h4>  <p>Learn to avoid the poppy fields and flying monkeys and WWW.</p>

        <h4><a href="#b-2-6-sample-class">2.6 Sampling from the UNet</a></h4> <p> Follow the yellow-brick road, but now with a helping hand from Glinda </p>

        <h4><a href="#b-nosched">Simplicity is best: getting rid of the learning rate scheduler.</a></h4>  <p>Old is gold; even without fancy choreography, the Yellow Brick Road leads home.</p>
    </section>



    <section class="section" id="proj5a">
        <div class="section-label">
            <span class="pill">
                <span class="dot"></span>
                ACT I · DIFFUSION SPELLS
            </span>
        </div>

        <h1>Project 5A: The Power of Diffusion Models</h1>
<h2>Part 0: Set Up</h2>
            <h3> Prompt Embeddings </h3>
            <figure class="side-by-side">  <img src="p5pt0/pembs.png" style="height:400px;" > <figcaption>After gaining access to the DeepFloyd IF diffusion model through Hugging Face, I generated prompt embeddings using the provided Hugging Face clusters. Here are some of the prompts I embedded. </figcaption> </figure>

            <h3> Model Outputs: <code>num_inference_steps = 20</code> </h3>
                <p>  Random seed used: <code> seed_everything(100) </code> </p>
                <div class="image-row"> <figure> <img src = "p5pt0/seed100/download.png">  <figcaption> Prompt: a sunflower wearing sunglasses </figcaption> </figure>
                                        <figure> <img src = "p5pt0/seed100/download-1.png">  <figcaption> Prompt: a beach with an umbrella </figcaption> </figure>
                                        <figure> <img src = "p5pt0/seed100/download-2.png">  <figcaption> Prompt: a volleyball team playing basketball </figcaption> </figure>  </div>

            <h3> Model Outputs: <code>num_inference_steps = 60</code> </h3>
                <p>  Random seed used: <code> seed_everything(100) </code> </p>
                <div class="image-row"> <figure> <img src = "p5pt0/seed100/stepsize 60/download.png">  <figcaption> Prompt: a sunflower wearing sunglasses </figcaption> </figure>
                                        <figure> <img src = "p5pt0/seed100/stepsize 60/download-1.png">  <figcaption> Prompt: a beach with an umbrella </figcaption> </figure>
                                       <figure> <img src = "p5pt0/seed100/stepsize 60/download-2.png">  <figcaption> Prompt: a volleyball team playing basketball </figcaption> </figure>  </div>

            <h3> Reflection </h3>
                <p> The "a sunflower wearing glasses" prompt at low <code> num_inference_steps </code> doesn't capture the image well. It often fails to generate the sunglasses, but is able to capture the sunfloweriness of the sunflower just fine. At higher <code>num_inference_steps</code>, however, it is able to do create the sunflower wearing sunglasses, and at a higher quality. </p>
                <p> The "a beach with an umbrella" prompt seems to do well with both higher and lower <code> num_inference_steps </code>. In both images, we are able to make out the beach through the presence of sand and water at the horizon, and a umbrella. Interestingly, both images resemble digitally drawn backdrops. Both are relatively high quality, and retrieve what the prompt is trying to achieve. </p>
                <p> The "a volleyball team playing basketball" prompt does worse than the other two, but is still passable as a realistic image if not observed closely. With a lower <code> num_inference_steps </code>, we see that the humans are more unnaturally placed, and there is no basketball to be seen, but there is an absense of a volleyball net and the girls don't really seem like they're playing volleyball, but are wearing volleyball uniforms. But with a higher <code> num_inference_steps </code>, we see a very low net, a basketball, and girls in volleyball attire/poses, but they don't seem to be playing basketball. Both images seem natural and seem to conform to the given prompt at a glance, but don't correspond to the prompt the same way the other prompts do. </p>


<h2>Part 1: Sampling Loops</h2>
    <h3>1.1 Implementing the Forward Process</h3>
            <h4> Goal: Implement <code> noisy_im = forward(im, t) </code> </h4>
                <p> This <code>forward</code> function implements a one forward diffusion step in a diffusion model, as it's useful in simulating progressively adding noise to a clean image over time. It takes a clean image tensor <code>im</code> of shape <code>(1, 3, 64, 64)</code> and a timestep <code>t</code>, then returns a noisy version of the clean image at that timestep. </p>
                <figure> <img src = "p5 pt1/pt1.1/formula.png" > </figure>
                <p> First, we sample <code>epsilon</code> from Gaussian N(0,1) noise (same shape as the image). Then, we get cumulative products of diffusion alphas at timestep <code>t</code>, <code>alpha_bar_t</code>, to determine how much of the original image should by incorporated. The clean image is scaled by <code>sqrt(alpha_bar_t)</code> and the noise is scaled by <code>sqrt(1 - alpha_bar_t)</code>, and their sum produces a noisy image <code>im_noisy</code>. </p>
            <h4> Implementation </h4> <figure> <img src = "p5 pt1/pt1.1/imp.png" > </figure>

            <h4> Campanile at Noise Levels <code>t = 250, 500, 750 </code> </h4>
                        <div class="image-row"> <figure> <img src = "p5 pt1/pt1.4/original.png">  <figcaption> Original (scaled) Berkeley Campanile </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/pt1.1/camp250.png">  <figcaption> Noisy Campanile at t=250 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/pt1.1/camp500.png">  <figcaption> Noisy Campanile at t=500 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/pt1.1/camp750.png">  <figcaption> Noisy Campanile at t=750 </figcaption> </figure>  </div>

    <h3>1.2 Classical Denoising</h3>
            <h4> Goal: Gaussian blur filtering to try to remove the noise using <code>torchvision.transforms.functional.gaussian_blur</code> with <code>kkernel_size = [7, 7]</code> and <code> sigma = 1</code>. </h4>
                        <div class="image-row"> <figure> <img src = "p5 pt1/pt1.1/camp250.png">  <figcaption> Noisy Campanile at t=250 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/pt1.1/camp500.png">  <figcaption> Noisy Campanile at t=500 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/pt1.1/camp750.png">  <figcaption> Noisy Campanile at t=750 </figcaption> </figure>  </div>
                        <div class="image-row"> <figure> <img src = "p5 pt1/1.2/kernelsize5/download.png">  <figcaption> Gaussian Blur Denoising at t=250 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/1.2/kernelsize7/download-1.png">  <figcaption> Gaussian Blur Denoising at t=500 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/1.2/kernelsize7/download-2.png">  <figcaption> Gaussian Blur Denoising at t=750 </figcaption> </figure>  </div>

    <h3>1.3 One-Step Denoising</h3>
            <p> Here, we aim to denoise using our model. First, we run the forward process on the image at the given timestep <code>t</code>, and then predict the noise using the pretrained denoiser. Now, we can generate the clean estimate by solving for <code>x_0</code> from the equation in Part 1.1. </p>
            <h4> Reminder of original (scaled) image: </h4> <figure> <img src = "p5 pt1/pt1.4/original.png" style: width="200">  </figure>
            <h4> Goal: Denoise Using the Denoiser in one step. </h4>
                        <div class="image-row"> <figure> <img src = "p5 pt1/pt1.1/camp250.png">  <figcaption> Noisy Campanile at t=250 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/pt1.1/camp500.png">  <figcaption> Noisy Campanile at t=500 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/pt1.1/camp750.png">  <figcaption> Noisy Campanile at t=750 </figcaption> </figure>  </div>
                        <div class="image-row"> <figure> <img src = "p5 pt1/1.3/y250/download-1.png">  <figcaption> Model Noise Estimate at t=250 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/1.3/y500/download-1.png">  <figcaption> Model Noise Estimate at t=500 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/1.3/y750/download-1.png">  <figcaption> Model Noise Estimate at t=750 </figcaption> </figure>  </div>     
                        <div class="image-row"> <figure> <img src = "p5 pt1/1.3/y250/download copy.png">    <figcaption> One-Step Denoised Campanile at t=250 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/1.3/y500/download-2.png">  <figcaption> One-Step Denoised Campanile at t=500 </figcaption> </figure>
                                                <figure> <img src = "p5 pt1/1.3/y750/download-2.png">  <figcaption> One-Step Denoised Campanile at t=750 </figcaption> </figure>  </div>                        

    <h3>1.4 Iterative Denoising </h3>
            <p> Instead of taking one step towards the clean estimate, we can take multiple steps toward the clean estimate of the given noisy image in iterative denoising. </p>
            <p> Essentially, we are trying to implement this function: </p>
            <h4>Create <code>strided_timesteps</code>: a list of monotonically decreasing timesteps, starting at 990, with a stride of 30, eventually reaching 0. </h4>                 <figure> <img src = "p5 pt1/pt1.4/strides.png" > </figure>
            <h4> Also initialize the timesteps using <code></code>stage_1.scheduler.set_timesteps(timesteps=strided_timesteps)</code> </h4>
            <h4>Complete the <code>iterative_denoise</code> function:</h4>         <p> Essentially, we are trying to implement this equation: </p>    <figure class="side-by-side">  <img src="p5 pt1/pt1.4/formula.png" style="height:100;" > <figcaption> • <code>x_t</code> is your image at timestep <code>t</code> <br> 
                                                                                                                                                                                             • <code>x_t'</code> (<code>t < t'</code>) is your noisy image at timestep <code>t'</code> (a less noisy image) <br>
                                                                                                                                                                                             • <code>alpha_bar_t</code> helps determine how much of the clean image we want to retain at time <code>t</code>
                                                                                                                                                                                             • <code>alpha_t</code> =  <code>alpha_bar_t</code> / <code>alpha_bar_t'</code> <br>
                                                                                                                                                                                             • <code>beta_t</code> =  <code>1 - alpha_t</code> <br>
                                                                                                                                                                                             • <code> x_0 </code> is our current estimate of the clean image <br>
                                                                                                                                                                                             • <code> v_sigma </code> is random noise predicted by DeepFloyd. </figcaption> </figure>
            <h4> Implementation </h4>                 <figure> <img src = "p5 pt1/pt1.4/imp.png" > </figure> <p> I collected alpha_bar_t and alpha_bar_t_prime from the scheduler. Then, I used the model to produce a noise estimate and predicted variance. To calculate the clean estimate, I rearranged the formula frmo part 1.1 and solved for x_0. Then, to calculate x_t_prime, I used the clean estimate and the alpha/beta terms I found/calculated, according to the equation above. I then also added the variance term. Repeating this many times, I am able to iteratively denoise to attain better image predictions. </p>

            <h4> Show the noisy Campanile every 5th loop of denoising (gradually becomes less noisy). </h4>
                            <div class="image-row">         <figure> <img src = "p5 pt1/pt1.4/t690.png">    <figcaption> Iterative Denoising, Noisy Campanile at t = 690 </figcaption> </figure>
                                                            <figure> <img src = "p5 pt1/pt1.4/t540.png">    <figcaption> Iterative Denoising, Noisy Campanile at t = 540 </figcaption> </figure>
                                                            <figure> <img src = "p5 pt1/pt1.4/t390.png">    <figcaption> Iterative Denoising, Noisy Campanile at t = 390 </figcaption> </figure>
                                                            <figure> <img src = "p5 pt1/pt1.4/t240.png">    <figcaption> Iterative Denoising, Noisy Campanile at t = 240 </figcaption> </figure>
                                                            <figure> <img src = "p5 pt1/pt1.4/t90.png">    <figcaption> Iterative Denoising, Noisy Campanile at t = 90 </figcaption> </figure> </div>
            
            <h4> Cross-Denoising-Method Comparisons </h4>
                            <div class="image-row">         <figure> <img src = "p5 pt1/pt1.4/original.png">    <figcaption> Original (scaled) Campanile </figcaption> </figure>
                                                            <figure> <img src = "p5 pt1/pt1.4/iterativedenoising.png">    <figcaption> Iterative Denoising: final clean image prediction  </figcaption> </figure>
                                                            <figure> <img src = "p5 pt1/pt1.4/onestep.png">    <figcaption> One Step Denoising: predicted clean image using a single denoising step (looks worse than iterative denoising)</figcaption> </figure>
                                                            <figure> <img src = "p5 pt1/pt1.4/guassianblur.png">    <figcaption> Gaussian-Filter Denoising: predicted clean image using gaussian blurring </figcaption> </figure>  </div>

    <h3>1.5 Diffusion Model Sampling </h3> <p> Instead of denoising an image, I can enter in random noise and the <code>iterative_denoise</code> function will try to make sense of the noise, ultimately generating images from scratch.</p>
    
    <p> By starting at <code>im_noise</code> = a random noisy image of desired shape, <code> i_start = 0</code>, <code> prompt_embeds = prompt_embeds_dict["a high quality photo"]</code>, and <code>timesteps=strided_timesteps</code>, I sampled some images from pure noise. </p>
                                <h4> Sampled Images (couldn't choose just 5): </h4>
                                <div class="image-row">     <figure> <img src = "p5 pt1/pt 1.5/download-5.png">    </figure>
                                                            <figure> <img src = "p5 pt1/pt 1.5/download.png">     </figure>
                                                            <figure> <img src = "p5 pt1/pt 1.5/download-6.png">     </figure>
                                                            <figure> <img src = "p5 pt1/pt 1.5/download-9.png">   </figure> 
                                                            <figure> <img src = "p5 pt1/pt 1.5/download-13.png">   </figure> </div>
                                        <div class="image-row">     <figure> <img src = "p5 pt1/pt 1.5/download-2.png">    </figure>
                                                            <figure> <img src = "p5 pt1/pt 1.5/download-7.png">     </figure>
                                                            <figure> <img src = "p5 pt1/pt 1.5/download-12.png">     </figure>
                                                            <figure> <img src = "p5 pt1/pt 1.5/download-14.png">   </figure> </div>


    <h3>1.6 Classifier-Free Guidance (CFG)</h3>  <p> The images sampled in 1.5 aren't always the best. But using CFG, we can improve image quality by combining conditional and unconditional noise estimates in the diffusion process. The strength of guidance is controlled by scale, which determines <code> epsilon = uncond_epsilon + scale * (cond_epsilon - uncond_epsilon) </code>.</p> <figure> <img src = "p5 pt1/1.6/formula.png" > </figure>
           <p> The only thing that changes is the calculation of epsilonm which will incorporate the same conditioned noise  from <code> iterative_denoise </code> using <code>prombt_embeds</code>, as well as an unconditioned noise estimate from the model using <code>uncond_prompt_embeds</code> according to the equaiton above. </p>
    <h4> Goal: Implement <code>iterative_denoise_cfg</code>  </h4>  <figure> <img src = "p5 pt1/1.6/imp.png" > </figure>
                <p> We can then generate better-quality pictures using  <code>iterative_denoise_cfg</code>  with <code>im_noise</code> = a random noisy image of desired shape,<code> i_start = 0</code>, <code> prompt_embeds = prompt_embeds_dict["a high quality photo"]</code>, <code>uncond_prompt_embeds = prompt_embeds_dict['']</code>, <code>timesteps=strided_timesteps</code>, and <code>scale = 7</code>.</p>

            <h4> Images using <code>iterative_denoise_cfg</code> with <code> scale = 7 </code> = CFG scale = gamma. </h4>
                                    <div class="image-row">     <figure> <img src = "p5 pt1/1.6/download-2.png">    </figure> 
                                                                <figure> <img src = "p5 pt1/1.6/download-5.png">    </figure> 
                                                                <figure> <img src = "p5 pt1/1.6/download copy.png">    </figure> 
                                                                <figure> <img src = "p5 pt1/1.6/download-10.png">    </figure> 
                                                                <figure> <img src = "p5 pt1/1.6/download-3 copy.png">    </figure> </div>

    <h3> 1.7 Image-to-image Translation </h3>
            <h4> SD Edit: Campanile </h4>
                                    <div class="image-row">     <figure> <img src = "p5 pt1/p5 pt1.7/1.7/campanile/istart1.png">  <figcaption> SDEdit with <code>i_start = 1</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/campanile/istart3.png"> <figcaption> SDEdit with <code>i_start = 3</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/campanile/istart5.png">  <figcaption> SDEdit with <code>i_start = 5</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/campanile/istart7.png">  <figcaption> SDEdit with <code>i_start = 7</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/campanile/istart10.png"> <figcaption> SDEdit with <code>i_start = 10</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/campanile/istart20.png">   <figcaption> SDEdit with <code>i_start = 20</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/campanile/campanile.png"> <figcaption>  Original Campanile </figcaption>  </figure>  </div>

            <h4> SD Edit: Mountains </h4>
                                    <div class="image-row">     <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own3/istart1.png">  <figcaption> SDEdit with <code>i_start = 1</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own3/istart3.png"> <figcaption> SDEdit with <code>i_start = 3</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own3/istart5.png">  <figcaption> SDEdit with <code>i_start = 5</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own3/istart7.png">  <figcaption> SDEdit with <code>i_start = 7</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own3/istart10.png"> <figcaption> SDEdit with <code>i_start = 10</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own3/istart20.png">   <figcaption> SDEdit with <code>i_start = 20</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own3/download.png"> <figcaption>  Original  </figcaption>  </figure>  </div>

                        
            <h4> SD Edit: Man </h4>
                                    <div class="image-row">     <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own2/1.png">  <figcaption> SDEdit with <code>i_start = 1</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own2/2.png"> <figcaption> SDEdit with <code>i_start = 3</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own2/3.png">  <figcaption> SDEdit with <code>i_start = 5</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own2/4.png">  <figcaption> SDEdit with <code>i_start = 7</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own2/5.png"> <figcaption> SDEdit with <code>i_start = 10</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own2/6.png">   <figcaption> SDEdit with <code>i_start = 20</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7/own2/download.png"> <figcaption>  Original  </figcaption>  </figure>  </div>
<h3> 1.7.1 Editing Hand-Drawn and Web Images </h3>
         <h4> One Image from the Web </h4>
                                        <div class="image-row"> <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/web/1.png">  <figcaption> Flower with <code>i_start = 1</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/web/3.png"> <figcaption> Flower with <code>i_start = 3</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/web/5.png">  <figcaption> Flower with <code>i_start = 5</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/web/7.png">  <figcaption> Flower with <code>i_start = 7</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/web/10.png"> <figcaption> Flower with <code>i_start = 10</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/web/20.png">   <figcaption> Flower with <code>i_start = 20</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/web/download.png"> <figcaption>  Original  </figcaption>  </figure>  </div>

         <h4> First hand-drawn image </h4>
                                        <div class="image-row"> <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw1/1.png">  <figcaption> Room with <code>i_start = 1</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw1/2.png"> <figcaption> Room with <code>i_start = 3</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw1/3.png">  <figcaption> Room with <code>i_start = 5</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw1/4.png">  <figcaption> Room with <code>i_start = 7</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw1/5.png"> <figcaption> Room with <code>i_start = 10</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw1/6.png">   <figcaption> Room with <code>i_start = 20</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw1/download.png"> <figcaption>  Original  </figcaption>  </figure>  </div>

         <h4> Second hand-drawn image </h4>
                                        <div class="image-row"> <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw2/download-1.png">  <figcaption> Sunflower with <code>i_start = 1</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw2/download-2.png"> <figcaption> Sunflower with <code>i_start = 3</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw2/download-3.png">  <figcaption> Sunflower with <code>i_start = 5</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw2/download-4.png">  <figcaption> Sunflower with <code>i_start = 7</code></figcaption>   </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw2/download-5.png"> <figcaption> Sunflower with <code>i_start = 10</code></figcaption>    </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw2/download-6.png">   <figcaption> Sunflower with <code>i_start = 20</code></figcaption>  </figure> 
                                                                <figure> <img src = "p5 pt1/p5 pt1.7/1.7.1/draw2/download.png"> <figcaption>  Original  </figcaption>  </figure>  </div>

<h3> 1.7.2 Inpainting </h3>
        <p>    To implement Inpainting functionality, we need a diffusion model and a binary mask to allow us to edit an image by replacing content outside of masked regions of the image while keeping the image contents inside the mask intact. To do this, at each diffusion denoising loop, we want to update our image according to: </p> <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/formula.png" > </figure>
        <p> This ensures that the pixels outside the mask retain the original image content (with appropriate noise), while pixels inside the mask are replaced with new content generated by the model.</p>
        <h4> Implement <code>impaint</code></h4>
            <p> Code follows the same structure as <code>iterative_denoise_cfg</code>, but instead of <code>image = pred_prev_image</code>, I use the equation above:</p><figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/imp.png" > </figure>

        <h4> The Campanile Impainted 1</h4> <div class="image-row"> <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/camp/c.png">         <figcaption>  Campanile      </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/camp/m.png">         <figcaption>  Mask           </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/camp/q.png">         <figcaption>  Pixels to Change  </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/camp/download.png">  <figcaption>  Campanile Inpainted  </figcaption>  </figure>  </div>


        <h4> The Campanile Impainted 2</h4> <div class="image-row"> <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/camp2/c.png">         <figcaption>  Campanile      </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/camp2/m.png">         <figcaption>  Mask           </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/camp2/q.png">         <figcaption>  Pixels to Change </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/camp2/download.png">  <figcaption>  Campanile Inpainted  </figcaption>  </figure>  </div>

        <h4> The Sunset Road Impainted</h4> <div class="image-row"> <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/own1/c.png">       <figcaption>  Sunset Road      </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/own1/m.png">         <figcaption>  Mask           </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/own1/q.png">         <figcaption>  Pixels to Change  </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/own1/download-6.png">  <figcaption>  Sunset Road Inpainted  </figcaption>  </figure>  </div>


        <h4> The Sunflower Bouquet Impainted</h4> <div class="image-row"> <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/own2/c.png">       <figcaption>   Sunflower Bouquet    </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/own2/m.png">         <figcaption>  Mask           </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/own2/q.png">         <figcaption>  Pixels to Change  </figcaption>    </figure> 
                                                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.2/own2/download.png">  <figcaption>   Sunflower Bouquet Inpainted  </figcaption>  </figure>  </div>

<h3> 1.7.3 Text-Conditional Image-to-image Translation </h3> <p> Using the <code>prompt_embeds</code> parameter of <code>iterative_denoise_cfg </code>, we can guide the projection even more, but this time with text.</p>
            <h4> Edits of the Campanile: <code> prompt_embeds = prompt_embeds_dict["a rocket ship"]</code></h4><div class="image-row"> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/camp/download.png">  <figcaption> Rocket Ship at Noise Level 1           </figcaption>  </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/camp/download-1.png">  <figcaption>  Rocket Ship at Noise Level 3 </figcaption>  </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/camp/download-2.png"> <figcaption>   Rocket Ship at Noise Level 5                                   </figcaption>    </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/camp/download-3.png">  <figcaption>    Rocket Ship at Noise Level 7                                      </figcaption>   </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/camp/download-4.png">  <figcaption>    Rocket Ship at Noise Level 10                                          </figcaption>   </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/camp/download-5.png"> <figcaption>      Rocket Ship at Noise Level 20                                             </figcaption>    </figure> 
                                    <figure> <img src = "p5 pt1/pt1.4/original.png"> <figcaption>  Original Campanile  </figcaption>  </figure>  </div>

            <h4> Edits of the Sunset Road: <code> prompt_embeds = prompt_embeds_dict["a beach with an umbrella"]</code></h4><div class="image-row"> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own1/download.png">  <figcaption>  Noise Level 1           </figcaption>  </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own1/download-1.png">  <figcaption>  Noise Level 3 </figcaption>  </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own1/download-2.png"> <figcaption>   Noise Level 5                                   </figcaption>    </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own1/download-3.png">  <figcaption>    Noise Level 7                                      </figcaption>   </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own1/download-4.png">  <figcaption>  Noise Level 10                                          </figcaption>   </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own1/download-5.png"> <figcaption>      Noise Level 20                                             </figcaption>    </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own1/download copy.png"> <figcaption>  Original Sunset Road  </figcaption>  </figure>  </div>

            <h4> Edits of the Sunflower Bouquet: <code> prompt_embeds = prompt_embeds_dict["a bouquet of marigold flowers"]</code></h4><div class="image-row"> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own2/noise1.png">  <figcaption>  Noise Level 1           </figcaption>  </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own2/noise3.png">  <figcaption>  Noise Level 3 </figcaption>  </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own2/noise5.png"> <figcaption>   Noise Level 5                                   </figcaption>    </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own2/noise7.png">  <figcaption>    Noise Level 7                                      </figcaption>   </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own2/noise10.png">  <figcaption>  Noise Level 10                                          </figcaption>   </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own2/noise20.png"> <figcaption>      Noise Level 20                                             </figcaption>    </figure> 
                                    <figure> <img src = "p5 pt1/p5 pt1.7/1.7.3/own2/download.png"> <figcaption>  Original Sunflower Bouquet  </figcaption>  </figure>  </div>

<h3> 1.8 Visual Anagrams </h3>
<p> Visual Anagrams are optical illusions where an image reveals different scenes depending on its orientation. </p>

    <h4>Method</h4>
                <ol>
                    <li> First, denoise an image <code>x</code> at timestep <code>t</code> with prompt <code>prompt1</code> to get noise estimate <code>epsilon1</code>.</li>
                    <li> Then, flip <code>x</code> (if we want 180 degree illusions) and denoise with <code>prompt2</code> to get noise estimate <code>epsilon2</code>.</li>
                    <li> Next, flip the image back and average the two noise estimates:</li> <pre><code>epsilon = (epsilon1 + flip(epsilon2)) / 2</code></pre>
                    <li> Lastly, denoise using <code>epsilon</code> to update the image, as done before.</li>
                </ol>
    <h4> Implement <code> make_flip_illusion </code> </h4> <p> This is how I changed the main body of <code>make_flip_illusion</code> . </p> <figure> <img src = "p5 pt1/pt1.8/imp.png" > </figure>   
    <h4> "a beach" and "a tulip field" </h4>       <div class="image-row"> <figure> <img src = "p5 pt1/pt1.8/beachtulip/1.png">  <figcaption> Beach </figcaption>  </figure> 
                                                                           <figure> <img src = "p5 pt1/pt1.8/beachtulip/2.png"> <figcaption> Tulip Field (Flipped) </figcaption>    </figure> </div>
    <h4> "a happy family" and "a bouquet" </h4>       <div class="image-row"> <figure> <img src = "p5 pt1/pt1.8/fambouqet/1.png">  <figcaption> Happy Family </figcaption>  </figure> 
                                                                           <figure> <img src = "p5 pt1/pt1.8/fambouqet/2.png"> <figcaption> Bouquet (Flipped) </figcaption>    </figure> </div>
    <h4>  "an oil painting of an old man" and "an oil painting of people around a campfire" </h4>       <div class="image-row"> <figure> <img src = "p5 pt1/pt1.8/manfire/1.png">  <figcaption>  "an oil painting of an old man" </figcaption>  </figure> 
                                                                           <figure> <img src = "p5 pt1/pt1.8/manfire/2.png"> <figcaption> "an oil painting of people around a campfire" (Flipped) </figcaption>    </figure> </div>
    <h4>  "a puppy" and "a raven" </h4>       <div class="image-row"> <figure> <img src = "p5 pt1/pt1.8/1.png">  <figcaption>  "a puppy" </figcaption>  </figure> 
                                                                           <figure> <img src="p5 pt1/pt1.8/2.png" style="transform: rotate(180deg); "> <figcaption> "a raven" (Flipped) </figcaption>    </figure> </div>
    <h4>  "a fire" and "a rose" </h4>       <div class="image-row"> <figure> <img src = "p5 pt1/pt1.8/download-4.png">  <figcaption>  "a fire" </figcaption>  </figure> 
                                                                           <figure> <img src="p5 pt1/pt1.8/download-3.png "> <figcaption> "a rose" (Flipped) </figcaption>    </figure> </div>
    


<h3> 1.9 Hybrid Images </h3> <p> We can also create hybrid images by combining features from two different prompts: Low-frequency details come from one prompt, and high-frequency details come from the other. </p>

  <h4>Method</h4>
    <ol>
        <li>First, generate two noise estimates <code>epsilon1</code> and <code>epsilon2</code> using the two text prompts <code>prompt1</code> and <code>prompt2</code> with the UNet diffusion model.</li>
        <li> Then, extract low frequencies from one noise estimate using a low-pass filter (<code>torchvision.transforms.functional.gaussian_blur</code>), and high frequencies from the other using a high-pass filter (<code>img - torchvision.transforms.functional.gaussian_blur</code>).</li>
        <li> Use Gaussian blur with kernel size 33 and sigma 2. </li>
        <li>Next, combine them to form the final noise estimate:</li> <pre><code>epsilon = low_pass(epsilon1) + high_pass(epsilon2)</code></pre>
        <li>Lastly, denoise using <code>epsilon</code> to update the image, as done before.</li>
    </ol>

    <h4> Implement <code> make_hybrids </code> </h4> <p> This is how I changed the main body of <code>make_hybrids</code> . </p> <figure> <img src = "p5 pt1/1.9/imp.png" > </figure>   

    <h4> Cat (close) and Puppy (far) </h4> <div class="image-row">  <figure> <img src = "p5 pt1/1.9/catpuppy/1.png" >  <figcaption> Cat</figcaption> </figure> 
                                                                     <figure> <img src = "p5 pt1/1.9/catpuppy/1.png" style="width: 30%;">  <figcaption> Puppy</figcaption> </figure> 
                                                                     </div>

    <h4> Sunset (close) and Old Man (far) </h4> <div class="image-row"> <figure> <img src = "p5 pt1/1.9/man sunset/1.png" > <figcaption> Sunset</figcaption></figure> 
    <figure> <img src = "p5 pt1/1.9/man sunset/1.png" style="width: 30%;">  <figcaption>Old Man</figcaption> </figure> 
    </div>
    
    <h4> Witch (close) and Tree (far) </h4> <div class="image-row"> <figure> <img src = "p5 pt1/1.9/witchtree/1.png" >  <figcaption> Witch</figcaption></figure> 
<figure> <img src = "p5 pt1/1.9/witchtree/1.png" style="width: 30%;">   <figcaption> Tree</figcaption> </figure> 
  </div>


    <h4> Waterfalls (close) and Skull (far) </h4> <div class="image-row"> <figure> <img src = "p5 pt1/1.9/skull/1.png" > <figcaption> Waterfalls</figcaption></figure> 
    <figure> <img src = "p5 pt1/1.9/skull/1.png" style="width: 30%;">  <figcaption> Skull</figcaption> </figure> 
   </div>
    </section>

    <section class="section" id="proj5b">
        <div class="section-label">
            <span class="pill">
                <span class="dot"></span>
                ACT II · FLOW MATCHING CHARMS
            </span>
        </div>

<h1>Project 5B: Flow Matching from Scratch</h1>
    <h2>Part 1: Training a Single-Step Denoising UNet</h2> <h3> Goal: Denoise noisy image in one step, optimizing over L2 Loss</h3>
                                <h2>1.1 Implementing the UNet </h2> <p> I implemented a simpler version of the UNet architecture used in diffusion models. The UNet consists of an upsampling and downsampling structure with skip connections, allowing both local and global features to be captured. The model takes in a noisy image as input and attempts to denoise the image.</p> <figure> <img src = "part B pt1/une.png" > </figure>

                                <h2> 1.2 Using the UNet to Train a Denoiser  </h2> <p> To train the denoiser, I need to generate noisy z and clean x image pairs. To create the noisy image from the clean, I need to add noise proportional to sigma to the clean image. I sampled from the MNIST dataset, obtained Gaussian noise <code>epsilon</code> using <code>torch.randn_like</code>, and create a noisy z = x + epsilon * sigma .  I made sure to clamp the resulting values between 0 and 1 for better visualization.</p>  <h3>Visualizing the noising process: </h3> <figure> <img src = "part B pt1/pt1.2.png" > </figure>
                                
                                            <h2> 1.2.1 Training </h2>
                                                    <h3> Goal: Train a denoiser to denoisy noisy image <code>z</code> in one step, using L2 loss with <code> σ = 0.5 </code> applied to a clean image <code>x</code>. </h3>  <p> I trained the UNet model over a total of 5 epochs, using the AdamW optimizer to minimze L2 loss (MSE).</p> <p> Hyperparameters: </p> <ul> <li>batch_size = 256</li> <li>learning_rate = 1e-4</li> <li>noise_level = 0.5</li> <li>hidden_dim = 128</li> <li>num_epochs = 5</li> </ul>
                                                    <h3> Training Loss Curve </h3> <figure> <img src = "part B pt1/savedmodels/training_curve.png" style="width:800px;" >  <figcaption>Training Loss Curve: Low loss achieved </figcaption></figure> 
                                                    <h3> Sample results on the test set with noise level <code> σ = 0.5 </code> after the first and the fifth epoch </h3>
                                                    <h3> On Random Images</h3>
                                                        <div class="image-row">     <figure> <img src = "part B pt1/savedmodels/denoised_results_epoch_1.png" style="width:400px;">  <figcaption> Epoch 1 </figcaption>  </figure> 
                                                                                    <figure> <img src = "part B pt1/savedmodels/denoised_results_epoch_5.png" style="width:400px;">  <figcaption> Epoch 5 </figcaption>  </figure> </div>
                                                    
                                                    <h3> On Same Random Images</h3> <h4> To better compare the same test images' results at each saved epoch of the model: </h4>
                                                        <div class="image-row">     <figure> <img src = "part B pt1/savedmodels/denoised_results_epoch_comparable_1.png" style="width:400px;">  <figcaption> Epoch 1 </figcaption>  </figure> 
                                                                                    <figure> <img src = "part B pt1/savedmodels/denoised_results_epoch_comparable_5.png" style="width:400px;">  <figcaption> Epoch 5 </figcaption>  </figure> </div>
                                                    
<h2> 1.2.2 Out-of-Distribution Testing </h2>
                                                    <p> The denoiser I just trained was trained on images noised at noise level <code> σ = 0.5 </code>, and it doesn't generalize well when test images are noised at different noise levels (called 'out of distribution' test images). </p>
                                                    <h3> Sampled results on the test set with out-of-distribution noise levels: </h3>
                                                    <h4> Small noise levels: <code> σ = [0.0, 0.2] </code> </h4>
                                                            <div class = "image-row">   <figure> <img src = "part B pt1/OODTesting/ood_sigma_0.0.png" style="width:300px;">  <figcaption> Noise Level 0.0 </figcaption>  </figure> 
                                                                                    <figure> <img src = "part B pt1/OODTesting/ood_sigma_0.2.png" style="width:300px;">  <figcaption> Noise Level 0.2 </figcaption>  </figure> </div>

                                                    <h4> Noise levels similar around <code>σ = 0.5 </code>:  <code> σ = [0.4, 0.5, 0.6] </code>  </h4>
                                                            <div class = "image-row">   <figure> <img src = "part B pt1/OODTesting/ood_sigma_0.4.png" style="width:300px;">  <figcaption> Noise Level 0.4 </figcaption>  </figure> 
                                                                                    <figure> <img src = "part B pt1/OODTesting/ood_sigma_0.5.png" style="width:300px;">  <figcaption> Noise Level 0.5 </figcaption>  </figure> 
                                                                                    <figure> <img src = "part B pt1/OODTesting/ood_sigma_0.6.png" style="width:300px;">  <figcaption> Noise Level 0.6 </figcaption>  </figure></div>
                                        
                                                    <h4> High noise levels:  <code> σ = [0.8, 1.0] </code>  </h4>
                                                            <div class="image-row">   <figure> <img src = "part B pt1/OODTesting/ood_sigma_0.8.png" style="width:300px;">  <figcaption> Noise Level 0.8 </figcaption>  </figure> 
                                                                                    <figure> <img src = "part B pt1/OODTesting/ood_sigma_1.0.png" style="width:300px;">  <figcaption> Noise Level 1.0 </figcaption>  </figure> </div>
                                                    <p> The model clearly does not generalize well to high out of distribution noise levels, as it's not trained to learn them. </p>
                                        
                                            
                                            <h2> 1.2.3 Denoising Pure Noise </h2>   <p> In generation tasks, we feed models noise and expect them to generate meaningful outputs. I trained the UNet model on pure, random Guassian noise <code> z</code> implemented using <code> torch.rand_liken()</code>. I hope for the model to be able to denoise this noise to get clean image <code>x</code>. </p> <h3> Goal: Train a denoiser to denoise pure noise image <code>z</code> in one step, using L2 loss. </h3>  <p> I trained the UNet model over a total of 5 epochs, using the AdamW optimizer to minimze L2 loss (MSE).</p> <p> Hyperparameters: </p> <ul> <li>batch_size = 256</li> <li>learning_rate = 1e-4</li> <li>hidden_dim = 128</li> <li>num_epochs = 5</li> </ul>
                                                    <h3> Training Loss Curve </h3> <figure> <img src = "part B pt1/purenoise/training_curve.png" >  <figcaption>Training Loss Curve: Doesn't converge. </figcaption></figure> 
                                                    <h3> Sample results on pure noise after the first and the fifth epoch. </h3>   
                                                                <div class="image-row">   <figure> <img src = "part B pt1/purenoise/denoised_results_epoch_comparable_1.png" style="width:300px;">  <figcaption> After 1 epoch </figcaption>  </figure> 
                                                                                            <figure> <img src = "part B pt1/purenoise/denoised_results_epoch_comparable_5.png" style="width:300px;">  <figcaption> After 5 epochs </figcaption>  </figure> </div>
                                                                                            <p> As a note: Though the test image is displayed, it wasn't actually used to generate the noisy input to the model, only as a reference.     </p>
                                                                                
                                                    <h3> Patterns observed, Possible Explanations </h3>
                                                    <p> The generated outputs of the model is a consistent blend of all nine digits, clearly not resembling any one digit but not changing with different noise inputs. The model is attempting to average of all of the images it saw during training, as an attempt to minimize the L2 loss it used in training. 
                                                        This is common when training models with MSE loss with very noisy/volatile data, where the model attempts to overlook the task at hand and instead minimzes L2 loss with an average guess. The model would end up doing this if it found no good patterns in the data, as the sum of squared distances to all training examples is minimized by predicting the mean of all examples in that case. 
                                                        Our model is trained on pure noise, and isn't given a direction to help learn anything on the noise. So, on our model, the average best-guess output looks, naturally, like the blurry digit-accumulation generated above. </p>
                                                        <p> And since on the training curve we saw that there's little decrease in loss from Epoch 1 to Epoch 5, we don't expect the model to improve much between these two epochs and is why there's similar output results between the two. </p>

<h2>Part 2: Training a Flow Matching Model</h2>  <h3> Goal: Denoise noisy image iteratively, using flow matching.</h3> <p> We want to denoise an image iteratively, by using the model to predict the flow from noisy to clean, and moving along the flow. </p>

<h2>2.1 Adding Time Conditioning to UNet</h2> <p>We want to injet a scalar <code>t</code> into the model. I implemented this by adding to FCBlocks to my architecture, following the diagram below.</p> <figure> <img src = "part B pt2/timecondunet.png" > </figure>

<h2>2.2 Training the UNet</h2> <p> To train the UNet, I give the model a clean image x1 from the training set, randomly sample time t and noisy image x0, create noisy image <code>xt = (1-t) * x0 + t * x1</code> and have the model predict the flow from clean to noisy using the following loss function: </p>  <img src = "part B pt2/timecondloss.png" > </figure> <p> This objecting minimizes the loss between flows, not the literal pixel values of the predicted and actual image. </p>
                                            <p> I used the following algorithm to implement the <code>forward</code> step: </p> <img src = "part B pt2/alg1.png" > </figure> 
                                            <p> To train, I used a AdamW optimizer, as well as an exponential learning rate decay scheduler. </p>
                                            <p> Hyperparameters: </p> <ul> <li>batch_size = 64</li> <li>(starting) learning_rate = 1e-2</li><li>num_epochs = 10</li> <li>hidden_dim = 64</li> <li>scheduler gamma = 0.1**(1/num_epochs) </li> <li> in-dimensions to unet: 1 </li> </ul>

                                        <h3> Training loss: time-conditioned UNet over all 10 epochs. </h3> <figure> <img src = "part B pt2/addtimecond/training_curve.png" >  <figcaption>Training Loss Curve: Low loss achieved </figcaption></figure> <p> Loss quickly decreases and begins to converge across all 10 epochs. The faint green lines demark the end of an epoch. </p>
<h2> 2.3 Sampling from the UNet </h2> <p> In order to actually visualize my results, I need to sample from the UNet after time-conditioning. </p>  <p> First, I start with a noisy image x0 sampled from Gaussian noise. Then, from t = 0 (noisy) to t = 1 (clean), I iterate along the flow in small step sizes. In each timestep, I first compute the flow from our current estimate of the image, x_t, using the trained UNet. This gives both the magnitude and direction of the change needed to go from our current x_t towards clean x_1. To actually update, I use the following algorithm's method to move along the flow, to get a slightly cleaner version of x_t. </p> <figure> <img src = "part B pt2/alg2.png" > </figure>
                                                    <h3> Sampling results from the Time-Conditioned UNet for 1, 5, and 10 epochs: </h3>      <div class = "image-column">   <figure> <img src = "part B pt2/addtimecond/sampling_results_epoch_1.png" style="width:800px;">  <figcaption> Epoch 1 </figcaption>  </figure> 
                                                                                                                                                                            <figure> <img src = "part B pt2/addtimecond/sampling_results_epoch_5.png" style="width:800px;">  <figcaption> Epoch 5 </figcaption>  </figure> 
                                                                                                                                                                            <figure> <img src = "part B pt2/addtimecond/sampling_results_epoch_10.png" style="width:800px;">  <figcaption> Epoch 10 </figcaption>  </figure></div>
                                        
                                                    <p> Though the results aren't perfect, you can distinguish the digits by 10 epochs of training, showing our time-conditioned UNet is working as expected. </p>

<h2> 2.4 Adding Class-Conditioning to UNet </h2> <p>To control which digits are sampled, I conditioned the UNet on the digits 0-9. To implement this, I added two more FCBlocks to the UNet architecture that take in a class-conditioning vector <code>c</code>. I also implemented a 10% dropout to drop the class conditioning vector, so that the UNet still works when it's not conditioned on any class. Dropout was implemented with the use of a <code> mask </code> arguement in the UNet architecture, which was all 0s with probability <code> p_uncond = 0.1 </code> and all 1s otherwise. </code>. </p> <p>Notable additions: </p> <figure> <img src = "part B pt2/addclasscond/structure.png" > </figure>

<h2> 2.5 Training the UNet </h2> <p>Training the class-conditioned UNet is similar to the time-conditioned UNet above. The goal is still to train the model to predict the flows, but now class conditioned on the ten digits. But, the UNet model now has an additional arguement <code>c</code>, allowing periodic unconditional generation.</p> <p> I followed the following algorithm: </p><img src = "part B pt2/alg3.png" > </figure>
                                             <p> To train, I used a AdamW optimizer, as well as an exponential learning rate decay scheduler. </p>
                                            <p> Hyperparameters: </p> <ul> <li>batch_size = 64</li> <li>(starting) learning_rate = 1e-2</li><li>num_epochs = 10</li> <li>hidden_dim = 64</li> <li>scheduler gamma = 0.1**(1/num_epochs) </li> <li> in-dimensions to unet: 1 </li> </ul>

                                                                     <h3> Training loss: class-conditioned UNet over all 10 epochs. </h3> <figure> <img src = "part B pt2/addclasscond/training_curve.png" >  <figcaption>Training Loss, Per Iteration, On Class-conditioned UNet <code>num_epochs = 10</code>: Low loss achieved </figcaption></figure> <p> Loss quickly decreases and begins to converge across all 10 epochs. The faint green lines demark the end of an epoch. </p>
<h2> 2.6 Sampling from the UNet </h2> To actually see the results from the class-conditioned UNet, I sampled from the model using classifer-free guidance, with guidance scale <code> gamma = 5.0 </code>. <p> First, I start with a noisy image x0 sampled from Gaussian noise. Then, from t = 0 (noisy) to t = 1 (clean), I iterate along the flow in small stepsizes (stepsize = 1/T).Per iteration, I first predict the unconditioned flow <code>u_uncond</code> with the class conditioning vector <code>c</code> set to be a vector of all 0s. This predicts the flow vector ignoring target class. Then, I predict the conditioned flow <code>u_cond</code> with the class conditioning vector <code> c </code> to predict the flow vector for the desired class. Then, I apply Classifer-free guidance, using the formula in line 6. I find the guided flow vector <code> u = u_uncond + gamma * (u_cond - u_uncond) </code>. This pushes us toward the feature space of the desired class determined by <code> c</code> by a factor proportional to <code> gamma </code>. Then, we update x_t, our running prediction, by incorporating the guided flow vector. This repeats until t = 1 and we predict x_1.<br> This is the algorithm implemented in the samplinng process:</p> <figure> <img src = "part B pt2/alg4.png" > </figure>

                                                        <h3> Sampling results from the Class-Conditioned UNet for 1, 5, and 10 epochs: </h3>      <div class = "image-column">   <figure> <img src = "part B pt2/addclasscond/class_conditional_samples_epoch_1.png" style="width:800px;">  <figcaption> Epoch 1 </figcaption>  </figure> 
                                                                                                                                                                                 <figure> <img src = "part B pt2/addclasscond/class_conditional_samples_epoch_5.png" style="width:800px;">  <figcaption> Epoch 5 </figcaption>  </figure> 
                                                                                                                                                                                 <figure> <img src = "part B pt2/addclasscond/class_conditional_samples_epoch_10.png" style="width:800px;"> <figcaption> Epoch 10 </figcaption>  </figure></div>

<h2> Simplicity is best: getting rid of the learning rate scheduler. </h2> <p> I tried to maintain the same performance after removing the exponential learning rate scheduler. Learning rate schedulers have the intended purpose of controlling how fast a model learns across epochs, enabling more stable and accurate convergence. Early in training, step sizes are large and become smaller as training progresses, to avoid an overshooting of the minimum. Since the model's training loss with the scheduler consistently decreases over time, I accounted for the lack of a scheduler by just starting with a lower learning rate to begin with. I used the same model parameters as before, but now with learning rate 1e-3 instead. </p>
                                                                <h3> Comparing Training Loss: With and Without Scheduler </h3> <figure> <img src = "part B pt2/nosched/training_curve.png" >  <figcaption>Training Loss Curve Comparison: Both achieve low loss </figcaption></figure> 
                                                                <p> Both models achieve similar low loss after 10 epochs of training. 
                                                                    The model without the scheduler (blue) has a slightly more volatile loss curve and has a lower loss at initial epochs, but still decreases overall. The model trained with the scheduler achieves the same loss with more iterations, but is overall more smooth. </p>
                                                                <h3> No scheduler: Sampling results from the Class-Conditioned UNet for 1, 5, and 10 epochs: </h3>      <div class = "image-column">   <figure> <img src = "part B pt2/nosched/class_conditional_samples_epoch_1.png" style="width:800px;">  <figcaption> Epoch 1 </figcaption>  </figure> 
                                                                                                                                                                                                       <figure> <img src = "part B pt2/nosched/class_conditional_samples_epoch_5.png" style="width:800px;">  <figcaption> Epoch 5 </figcaption>  </figure> 
                                                                                                                                                                                                       <figure> <img src = "part B pt2/nosched/class_conditional_samples_epoch_10.png" style="width:800px;"> <figcaption> Epoch 10 </figcaption>  </figure></div>

<h2>Citations</h2>
<p> • Project:  project adapted from CS180 Project 5 (UC Berkeley). </p>
<p> • Website: Wicked and all of the AI that helped create this website. </p>





<h2> the end </h2>

    </section>
</div>

<footer>
    No one mourns the code that isn’t styled.<br>
    Polished with a little <span class="wicked-highlight">Glinda sparkle</span> and a hint of emerald.
</footer>
</body>
</html>

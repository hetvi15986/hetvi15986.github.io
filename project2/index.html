<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 1</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 20px auto;
            text-align: left;
        }
        h1 {
            color: #530f0f;
            text-align: center;
            
        }
        h2 {
            color: #8e1600;
            text-align: center;
            margin-top: 20px;
        }
        h3 {
            color: #804141;
            margin: 20px 10px 10px 10px;
            text-align: center;
        }
        h4 {
            color: #664848;
            margin: 20px 10px 10px 10px;
        }
        h5 {
            color: #a26b6b;
            margin: 20px 10px 10px 10px;
        }
        p {
            margin: 20px 10px 10px 10px;
        }
        
        img {
            max-width: 400px;
            height: auto;
            margin: 20px 10px 10px 10px;

        }

        figure {
            text-align: center; /* centers both image and caption */
            margin: 20px 0;
        }
        figcaption {
            font-family: 'Open Sans', sans-serif;
            font-size: 14px;
            color: #555;
            margin-top: 5px;
        }
        .image-row {
            display: flex;         
            gap: 20px;             
            flex-wrap: wrap;       
            justify-content: center; 
        }
        .image-row figure {
            margin: 0;
            text-align: center;
            flex: 0 1 200px;       
        }
        .image-row img {
            max-width: 100%;
            height: auto;
            display: block;
        }
        .image-column {
            display: flex;
            flex-direction: column; 
            gap: 20px;      
            align-items: center;
        }

        .image-column figure {
            margin: 0;
            text-align: center;
        }

        .image-column img {
            max-width: 90%;  
            height: auto;
            display: block;
        }

    </style>
</head>

<body>
<h1>Project 2: Fun With Filters and Frequencies</h1>

<h2> Part 1: Fun with Filters</h2>


<h3> Part 1.1: Convolutions from Scratch! </h3>

<h4> A: Four For Loops </h4>

    <p>Idea: Convolve img with a kernel using four loops: one for every x,y in image and every u,v in kernel.</p>
    <figure>
        <img src="pt1.1/fourloops.png" alt="full code" width="400">
        <figcaption> Four For Loops Convolution </figcaption>
    </figure>
    <p>Due to zero-padding, calculating the kernels/image offsets was pretty tricky!</p>
    <figure>
        <img src="pt1.1/four_ss.png" alt="padding indexing" width="400">
        <figcaption> Four For Loops, showing padding index offsets </figcaption>
    </figure>

<h4> B: Two For Loops </h4>

    <p>Idea: Vectorization Idea: Convolve img with a kernel using two loops: one for every x,y in image, by directly calculating the dot product between the kernel and image patch that corresponds to the ith pixel in the desired output image.</p>
    <figure>
        <img src="pt1.1/two_ss.png" alt="Sample Image" width="400">
        <figcaption> Two For Loops Convolution </figcaption>
    </figure>

<h4> C: Use built-in convolution function scipy.signal.convolve2d </h4>

    <p>Idea: Convolve img with the built-in function nested in another to aid in calculating running time, and compare outputs and runtimes.</p>
    <figure>
        <img src="pt1.1/builtin_ss.png" alt="Sample Image" width="400">
        <figcaption> Two For Loops Convolution </figcaption>
    </figure>

<h4> D: Results for picture of me:   </h4>
    <p> Idea: Convolve a grayscale image of myself with the functions, and compare outputs and runtimes. </p>
    <h5> 9x9 Box Filter with all three filters runtimes </h5>
        <div class="image-row">
            <figure> <img src="pt1.1/hp.png"> <figcaption>original image</figcaption> </figure>
            <figure>
                <img src="pt1.1/output_images/hp_gray.jpeg">
                <figcaption> Grayscale version </figcaption>
            </figure>
            <figure>
                <img src="pt1.1/output_images/hp_four_box.png">
                <figcaption> 9x9 Box Filter Convolution </figcaption>
                <figcaption> Four Forloops </figcaption>
                <figcaption> 23.0152 seconds </figcaption>
            </figure>
            <figure>
                <img src="pt1.1/output_images/hp_two_box.png">
                <figcaption> 9x9 Box Filter Convolution </figcaption>
                <figcaption> Two Forloops </figcaption>
                <figcaption> 3.6689 seconds</figcaption>
            </figure>
            <figure>
                <img src="pt1.1/output_images/hp_builtin_box.png">
                <figcaption>9x9 Box Filter Convolution </figcaption>
                <figcaption> Builtin function </figcaption>
                <figcaption> 0.083507 seconds </figcaption>
            </figure>
        </div>

    <h5> Convolve with finite difference operator Dx with all three filters runtimes </h5>
        <div class="image-row">
            <figure> <img src="pt1.1/hp.png"> <figcaption>original image</figcaption> </figure>
            <figure> <img src="pt1.1/output_images/hp_gray.jpeg"> <figcaption> Grayscale version </figcaption> </figure>
            <figure>
                <img src="pt1.1/output_images/hp_four_Dx.png">
                <figcaption> Dx Convolution </figcaption>
                <figcaption> Four Forloops </figcaption>
                <figcaption> 1.5068 seconds </figcaption>
            </figure>
            <figure>
                <img src="pt1.1/output_images/hp_two_Dx.png">
                <figcaption> Dx Convolution </figcaption>
                <figcaption> Two Forloops </figcaption>
                <figcaption> 4.7630 seconds</figcaption>
            </figure>
            <figure>
                <img src="pt1.1/output_images/hp_builtin_Dx.png">
                <figcaption> Dx Convolution </figcaption>
                <figcaption> Builtin function </figcaption>
                <figcaption> 0.083507 seconds </figcaption>
            </figure>
        </div>

    <h5> Convolve with finite difference operator Dy with all three filters runtimes </h5>
        <div class="image-row">
            <figure> <img src="pt1.1/hp.png"> <figcaption>original image</figcaption> </figure>
            <figure> <img src="pt1.1/output_images/hp_gray.jpeg"> <figcaption> Grayscale version </figcaption> </figure>
            <figure>
                <img src="pt1.1/output_images/hp_four_Dy.png">
                <figcaption> Dy Convolution </figcaption>
                <figcaption> Four Forloops </figcaption>
                <figcaption> 1.6330 seconds </figcaption>
            </figure>
            <figure>
                <img src="pt1.1/output_images/hp_two_Dy.png">
                <figcaption> Dy Convolution </figcaption>
                <figcaption> Two Forloops </figcaption>
                <figcaption> 4.1852 seconds</figcaption>
            </figure>
            <figure>
                <img src="pt1.1/output_images/hp_builtin_Dy.png">
                <figcaption> Dy Convolution </figcaption>
                <figcaption> Builtin function </figcaption>
                <figcaption> 0.014038 seconds </figcaption>
            </figure>
        </div>

        <p>We can see that Dx highlights edges vertical going separating left from right (For example, the light on my arm, campanile, building windows), while Dy highlights the horizontal edges (railings behind me).</p>
        <p>First, I padded boundaries to be 0. We can see the result of this on the edges of my images, where there's a black blurring on the edges. </p>
        <p> We can also see that the runtimes between the three identical-result-producing methods are drastically different, as seen with the use of a time() function to calculate how much time elapses from the start to the end of a function call. In the image cases, the Implementation A) Four For loops takes 24 seconds, while implementation B) Two For Loops takes 3 seconds (decrease by a factor of 8), while Implementation C) Built In scipy.Signal.convolve2d takes 0.07 seconds (decrease by a factor over 40), likely due to some built-in parallelization schemes. In conclusion: parallelization is great for performance on image transformations. </p>
    <h4> numerical check: Test matrix from discussion </h5>
        <div class="image-row">
            <figure> <img src="pt1.1/mat.png"> <figcaption> original array </figcaption> </figure>
            <figure> <img src="pt1.1/test_box.png"> <figcaption> test 3x3 box filter </figcaption> </figure>
            <figure> <img src="pt1.1/test_DxDy.png"> <figcaption> test Dx/Dy convolution </figcaption> </figure>
        </div>



<h3> Part 1.2: Finite Difference Operator </h3>
<p>First, I find the partial derivatives in x and y of the cameraman image by convolving the image with finite difference operators D_x and D_y. Still using zeropadding, with the built in function. </p>
<p>The gradient is a vector of both the partial derivatives in x and y. As a result, the magnitude of the gradient can be found using the Pythagorean thrm, as the square root of the sum between the squared components Dx and Dy.</p>
        <div class="image-row">
            <figure> <img src="pt1.2/cameraman.png"> <figcaption>original image</figcaption> </figure>
            <figure> <img src="pt1.2/cameraman_gray.png"> <figcaption> (2-channel) grayscale </figcaption> </figure>
        </div>
        <div class="image-row">
            <figure> <img src="pt1.2/cameraman_Dx.png"> <figcaption> Convolve with Dx </figcaption> </figure>
            <figure> <img src="pt1.2/cameraman_Dy.png"> <figcaption> Convolve with Dy </figcaption> </figure>
            <figure> <img src="pt1.2/cameraman_grads.png"> <figcaption> gradient (Dx,Dy) magnitudes </figcaption> </figure>
        </div>
        <p>  Now, I'll turn this into an edge image by binarizing the gradient magnitude and picking the best threshold that suppresses noise while showing all the real edges. This means that I exclude gradient magnitudes that are below my threshold in order to reduce noise. When doing this, I found a range of thresholds that give me different types of information. With a threshold value of 0.22, I see all of the edges, some of the background edges that give me more contextual information, and there is some noise along the bottom of the image. With a threshold of 0.27, I no longer have the noise along the bottom, the camera is still distinguishably a camera man, but some background information and edges are lightened. With the threshold of 0.32, the noise is largely gone- leaving mostly real edges, but some of the less distinct edges are gone (such as the ones from the back of his coat), and there's no decipherablebackground information left. Each of these thresholds show the tradeoff between trying to get the clearest edges while filtering out the noise in the event that faint information can be mistaken for noise.    </p>
        <div class="image-row">
            <figure> <img src="pt1.2/cameraman_edges_0.22.png"> <figcaption> threshold = 0.22 (some noise, more info) </figcaption> </figure>
            <figure> <img src="pt1.2/cameraman_edges_0.27.png"> <figcaption> threshold = 0.22 (little noise, retains info) - my optimal choice  </figcaption> </figure>
            <figure> <img src="pt1.2/cameraman_edges_0.32.png"> <figcaption> threshold = 0.22 (less noise, less info)  </figcaption> </figure>
        </div>


<h3> Part 1.3: Derivative of Gaussian (DoG) Filter </h3>
        <p>The results with just the difference operator were noisy, and we had to do a lot of tradeoffs between keeping distinct edges and removing noise. Instead, we can smooth the cameraman image by convolving the cameraman image with a Guassian filter, and then isolating the edges as before.</p>
        <p>First, create a 2D gaussian filter is by using cv2.getGaussianKernel() to create a 1D gaussian and then taking an outer product with its transpose to get a 2D gaussian kernel and convolve the cameraman iamge with it to get a blurred cammera man. </p>
        <p> g1d = cv2.getGaussianKernel(ks, sigma)</p>
        <p> gaussian_kernel = g1d @ g1d.T</p>
        <div class="image-row">
            <figure> <img src="pt1.2/cameraman.png"> <figcaption>original image</figcaption> </figure>
            <figure> <img src="pt1.2/cameraman_gray.png"> <figcaption> (2-channel) grayscale </figcaption> </figure>
            <figure> <img src="pt1.3/cameraman_blurred.png"> <figcaption> blurred and normalized grayscale cameraman image by convolving with the Guassian kernel </figcaption> </figure>
            <figure> <img src="pt1.3/Gaussian_kernel.png" style="width:500px; height:auto;" >  <figcaption> Guassian kernel (normalized) </figcaption> </figure>
        </div>
        <p>Then, find Dx, Dy, and gradient magnitudes.</p>
        <div class="image-row">
            <figure> <img src="pt1.3/cameraman_blurred_Dx.png"> <figcaption>Convolve with Dx</figcaption> </figure>
            <figure> <img src="pt1.3/cameraman_blurred_Dy.png"> <figcaption> Convolve with Dy </figcaption> </figure>
            <figure> <img src="pt1.3/cameraman_blurred_grads.png"> <figcaption>  gradient (Dx,Dy) magnitudes </figcaption> </figure>
        </div>
        <p>Now, I'll turn this into an edge image by binarizing the gradient magnitude and picking the best threshold that suppresses noise while showing all the real edges, just as before in part 1.2: Finite Difference Operator. </p>
        <p>When doing this, I found a range of thresholds that give me different types of information. With a threshold value of 0.15, I see all of the edges, and a lot more of the background features from the buildings edges, and some foreground noise. However, the edges also give a lot more information, and we might want to decrease what we see to focus on more clear parts of the image. With a threshold of 0.21, there's no longer any foreground noise and more clear pronounced background building windows. The man is still clearly indentifiable as a man looking through a camera, but the edges are more defined, even the fainter ones from when we found edges without the Guassian. With the threshold value of 0.27, the noise is largely gone, but some edges are less defined (for example, the edges outlining his legs are getting fainter, some camera-stand lines are less distinguishable).</p>
        <p>Each of these thresholds show the tradeoff between trying to get the clearest edges while filtering out the noise in the event that faint information can be mistaken for noise. The Gaussian, however, helps lessen the magnitude of this tradeoff, allowing for better edge detection. </p>
        <div class="image-row">
            <figure> <img src="pt1.3/cameraman_edges_0.15.png"> <figcaption> threshold = 0.15 (some noise, more info) </figcaption> </figure>
            <figure> <img src="pt1.3/cameraman_edges_0.21.png"> <figcaption> threshold = 0.21 (little noise, retains info) - my optimal choice  </figcaption> </figure>
            <figure> <img src="pt1.3/cameraman_edges_0.27.png"> <figcaption> threshold = 0.27 (less noise, less info)  </figcaption> </figure>
        </div>

        <p>We can do the same thing with a single convolution instead of two by creating a derivative of gaussian filters. Convolve the gaussian with D_x and D_y and display the resulting DoG filters as images. Note, I kept my Gaussian small as I liked the output of the cameraman picture. If they were larger, the initial images would be more blurred.</p>
        <p> Dx_G = convolve(gaussian_kernel, Dx)</p>
        <p> Dy_G = convolve(gaussian_kernel, Dy)</p>
        <p> Then, convolve img I with Dx_G and also with Dy_G. calculate the magnitudes between the two, and then binarize. </p>
        <p> We'd expect to get the same results.   </p>
        <div class="image-row">
            <figure> <img src="pt1.3/composed/DOGX.png" style="width:500px; height:auto;"> <figcaption> DOGx </figcaption> </figure>
            <figure> <img src="pt1.3/composed/DOGY.png" style="width:500px; height:auto;"> <figcaption> DOGy </figcaption> </figure>
        </div>
        <div class="image-row">
            <figure> <img src="pt1.3/composed/cameraman_blurred_Dx_G.png"> <figcaption> convolve image and DOGx  </figcaption> </figure>
            <figure> <img src="pt1.3/composed/cameraman_blurred_Dy_G.png"> <figcaption> convolve image and DOGy</figcaption> </figure>
            <figure> <img src="pt1.3/composed/cameraman_blurred_grads.png"> <figcaption> gradient magnitudes </figcaption> </figure>
        </div>
        <div class="image-row">
            <figure> <img src="pt1.3/composed/cameraman_edges_0.15.png"> <figcaption> threshold = 0.15 (some noise, more info) </figcaption> </figure>
            <figure> <img src="pt1.3/composed/cameraman_edges_0.21.png"> <figcaption> threshold = 0.21 (little noise, retains info) - my optimal choice  </figcaption> </figure>
            <figure> <img src="pt1.3/composed/cameraman_edges_0.27.png"> <figcaption> threshold = 0.27 (less noise, less info)  </figcaption> </figure>
        </div>
        <p> We got the same results.   </p>

<h2>Part 2: Fun with Frequencies</h2>
<h3> Part 2.1: Image "Sharpening" </h3>
<h3> i.e. Taj and Raj </h3>
    <p>Intuition: Attaining a blurred image (by applying the Gaussian on every channel and combininig) from the image by convolving with the Guassian is the same as passing a filter to attain the low frequencies of the image. To attain the high frequencies of the image, we would subtract the original image by the blurred image, subtracting away the lower frequencies. The human eye associates these high frequencies with detail, meaning this low-frequency-less image can look hyper detailed. But, we do want to still keep the low frequncies in the image, otherwise we're not really getting a good picture. We just want there to be more high frequencies than before, relative to the low frequencies. So, we can add the high frequncies to the original image, scaled by a factor of a constant alpha that control the degree of high frequencies (hence, sharpening) we'd like to insert into the original picture. This may not directly remove the low frequencies, but makes them a smaller proportion of the frequencies present in the new sharpened image. We can do this for every color channel individually, and then add the color channels back to attain the colored sharpened image.</p>
    <h4> Taj Mahal </h4>
        <div class="image-row">
            <figure> <img src="pt2.1/taj.jpg"> <figcaption> original image </figcaption> </figure>
            <figure> <img src="pt2.1/taj/taj_blurred.png"> <figcaption> blurred Taj Mahal </figcaption> </figure>
            <figure> <img src="pt2.1/taj/taj_high-frequency.png"> <figcaption> subtract the blurred Taj Mahal from the original, substracting away small frequencies from the image </figcaption> </figure>
        </div>
        <p> Now, I can sharpen by differing alpha </p>
        <div class="image-row">
            <figure> <img src="pt2.1/taj/taj_sharp_alpha1.png"> <figcaption> alpha = 1 </figcaption> </figure>
            <figure> <img src="pt2.1/taj/taj_sharp_alpha2.png"> <figcaption> alpha = 2 </figcaption> </figure>
            <figure> <img src="pt2.1/taj/taj_sharp_alpha3.png"> <figcaption> alpha = 3 </figcaption> </figure>
        </div>
    <h4> Me and Raj </h4>
        <p> Replicating the same process as above with an old photograph that always looks a little blurry. </p>
        <div class="image-row">
            <figure> <img src="pt2.1/mine/b3.JPG"> <figcaption> original image </figcaption> </figure>
            <figure> <img src="pt2.1/b3/b3_blurred.png"> <figcaption> blurred (slight) </figcaption> </figure>
            <figure> <img src="pt2.1/b3/b3_high-frequency.png"> <figcaption> subtract the blurred Taj Mahal from the original, substracting away small frequencies from the image </figcaption> </figure>
        </div>
        <p> Now, I can sharpen by differing alpha </p>
        <div class="image-row">
            <figure> <img src="pt2.1/b3/b3_sharp_alpha1.png"> <figcaption> alpha = 1 </figcaption> </figure>
            <figure> <img src="pt2.1/b3/b3_sharp_alpha2.png"> <figcaption> alpha = 2 </figcaption> </figure>
            <figure> <img src="pt2.1/b3/b3_sharp_alpha3.png"> <figcaption> alpha = 3 </figcaption> </figure>
        </div>
        <p> You can attain the same results by doing a single convolution per channel by creating an unsharpening kernel (1+alpha)*identity - alpha*gaussian_kernel (as seen in lecture) </p>
        <div class="image-row">
            <figure> <img src="pt2.1/formula.png" style="width:500px; height:auto;" > <figcaption> unsharp mask filter (sharpening filter) </figcaption> </figure>
        </div>
        <div class="image-row">
            <figure> <img src="pt2.1/b3_one/b3_sharp_alpha1.png"> <figcaption> alpha = 1 </figcaption> </figure>
            <figure> <img src="pt2.1/b3_one/b3_sharp_alpha2.png"> <figcaption> alpha = 2 </figcaption> </figure>
            <figure> <img src="pt2.1/b3_one/b3_sharp_alpha3.png"> <figcaption> alpha = 3 </figcaption> </figure>
            <figure> <img src="pt2.1/b3_one/b3_sharp_alpha5.png"> <figcaption> alpha = 4 </figcaption> </figure>
            <figure> <img src="pt2.1/b3_one/b3_sharp_alpha5.png"> <figcaption> alpha = 5 </figcaption> </figure>
        </div>


<h3> Part 2.2: Hybrid Images </h3>
        <h4> Derek and Nutmeg </h4>
        <div class="image-row">
            <figure> <img src="pt2.2/one/DerekPicture.jpg"> <figcaption> Derek </figcaption> </figure>
            <figure> <img src="pt2.2/one/nutmeg.jpg"> <figcaption> Nutmeg </figcaption> </figure>
        </div>
        <div class="image-row">
            <figure> <img src="pt2.2/one/fft/im1_fft.png"> <figcaption> Derek </figcaption> </figure>
            <figure> <img src="pt2.2/one/fft/im2_fft.png"> <figcaption> Nutmeg </figcaption> </figure>
        </div>


        <p> First, I aligned them using given helper functions. </p>
        <div class="image-row">
            <figure> <img src="pt2.2/one/im1_aligned.png"> <figcaption> Derek, aligned </figcaption> </figure>
            <figure> <img src="pt2.2/one/im2_aligned.png"> <figcaption> Nutmeg, aligned </figcaption> </figure>
        </div>
        <div class="image-row">
            <figure> <img src="pt2.2/one/fft/im1_aligned_fft.png"> <figcaption> Derek, aligned </figcaption> </figure>
            <figure> <img src="pt2.2/one/fft/im2_aligned_fft.png"> <figcaption> Nutmeg, aligned </figcaption> </figure>
        </div>


        <p> I grayscale both images. Then, I attained the low frequencies of Derek's image using a Gaussian kernel, since I want him to be seen from far. 
            I also attained the high frequencies of Nutmeg, since I want to see Nutmeg from up close, by convolving the image with unit impulse kernel minus the Gaussian. </p>
        <div class="image-row">
            <figure> <img src="pt2.2/one/im1_aligned_lowfreq.png"> <figcaption> Derek, aligned and blurred (low frequency) </figcaption> </figure>
            <figure> <img src="pt2.2/one/im2_aligned_highfreq.png"> <figcaption> Nutmeg, aligned and 'sharp' (high frequency) </figcaption> </figure>
        </div>
            <figure> <img src="pt2.2/one/fft/im1_aligned_lowfreq_fft.png"> <figcaption> Derek, aligned and blurred (low frequency) </figcaption> </figure>
            <figure> <img src="pt2.2/one/fft/im2_aligned_highfreq_fft.png"> <figcaption> Nutmeg, aligned and 'sharp' (high frequency) </figcaption> </figure>
        </div>
        <h5> Upclose: Nutmeg (cat) , Far: Derek (owner) </h5>
        <p> Then, I blended the two together by adding them. I chose cutoff frequencies by trial and error, looking at the images, though the cutoff is reflected by the difference of wavelength distribution in the fourier transform graphs as well. </p>
        </div>
            <figure> <img src="pt2.2/one/hybrid.png"> <figcaption> Hybrid </figcaption> </figure>
            <figure> <img src="pt2.2/one/fft/hybrid_fft.png"> <figcaption> Hybrid, Fourier </figcaption> </figure>
        </div>

        <h4> Then and Now: Dolly Parton </h4>
        <h5> Upclose: Dolly Parton today, Far: Dolly Parton then </h5>
        <div class="image-row">
            <figure> <img src="pt2.2/three/then.png"> <figcaption> Far: Dolly Parton then (low freq) </figcaption> </figure>
            <figure> <img src="pt2.2/three/today.png"> <figcaption> Close: Dolly Parton now (high freq)  </figcaption> </figure>
            <figure> <img src="pt2.2/three/hybrid.png"> <figcaption> Hybrid Image </figcaption> </figure>

        </div>

        <h4> "Then and Now": Spidermans</h4>
        <h5> Upclose: Tom Holland, Far: Andrew Garfield </h5>
        <div class="image-row">
            <figure> <img src="pt2.2/two/ag.jpg"> <figcaption> Far: Andrew Garfield (low freq) </figcaption> </figure>
            <figure> <img src="pt2.2/two/th.jpg"> <figcaption> Close: Tom Holland (high freq)  </figcaption> </figure>
            <figure> <img src="pt2.2/two/hybrid.png"> <figcaption> Hybrid Image </figcaption> </figure>

        </div>


<h2> Multi-resolution Blending and the Oraple journey </h2>
<h3> Part 2.3: Gaussian and Laplacian Stacks </h3>
<p> Here, we implement Gaussian and Laplacian Stacks from Scratch. </p>
        <h4> Orange + Apple = Oraple </h4>

        <div class="image-row">
            <figure> <img src="pt2.4/spline/apple.jpeg"> <figcaption> Apple </figcaption> </figure>
            <figure> <img src="pt2.4/spline/final.png"> <figcaption> Blend (regular straight mask)  </figcaption> </figure>
            <figure> <img src="pt2.4/spline/orange.jpeg"> <figcaption> Orange </figcaption> </figure>

        </div>

        <h5> How? Depths = 9, sigma = 1 </h5>
        
        <figure> <img src="pt2.4/spline/avg_contributions_0.png" > <figcaption> depth: 0 </figcaption> </figure>
        <figure> <img src="pt2.4/spline/avg_contributions_1.png" > <figcaption> depth: 1 </figcaption> </figure>
        <figure> <img src="pt2.4/spline/avg_contributions_2.png" > <figcaption> depth: 2 </figcaption> </figure>
        <figure> <img src="pt2.4/spline/avg_contributions_3.png" > <figcaption> depth: 3 </figcaption> </figure>
        <figure> <img src="pt2.4/spline/avg_contributions_4.png" > <figcaption> depth: 4 </figcaption> </figure>
        <figure> <img src="pt2.4/spline/avg_contributions_5.png" > <figcaption> depth: 5 </figcaption> </figure>
        <figure> <img src="pt2.4/spline/avg_contributions_6.png" > <figcaption> depth: 6 </figcaption> </figure>
        <figure> <img src="pt2.4/spline/avg_contributions_7.png" > <figcaption> depth: 7 </figcaption> </figure>
        <figure> <img src="pt2.4/spline/avg_contributions_8.png" > <figcaption> depth: 8 </figcaption> </figure>
    
    
        <h5> Average Contributions Summed: </h5>
        <figure> <img src="pt2.4/spline/avg_contributions_summed.png" > <figcaption> Sum </figcaption> </figure>

        <h5> Cummulative Blend: </h5>
        <figure> <img src="pt2.4/spline/cumulative_blend_strip.png" > <figcaption> Blending over time </figcaption> </figure>



<h3> Part 2.4: Multiresolution Blending (a.k.a. the oraple!) </h3>

<h4> I Am Iron Man </h4>
        <h5> Iron Man Suit and Iron Man, regular straight mask </h5>

        <div class="image-row">
            <figure> <img src="pt2.4/mcu/ironman.png"> <figcaption> Iron Man </figcaption> </figure>
            <figure> <img src="pt2.4/mcu/final.png"> <figcaption> Blend (regular straight mask)  </figcaption> </figure>
            <figure> <img src="pt2.4/mcu/tony.png"> <figcaption> Tony Stark </figcaption> </figure>

        </div>

        <h5> How? Depths = 11, sigma 0.5 </h5>
        
        <figure> <img src="pt2.4/mcu/avg_contributions_0.png" > <figcaption> depth: 0 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_1.png" > <figcaption> depth: 1 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_2.png" > <figcaption> depth: 2 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_3.png" > <figcaption> depth: 3 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_4.png" > <figcaption> depth: 4 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_5.png" > <figcaption> depth: 5 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_6.png" > <figcaption> depth: 6 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_7.png" > <figcaption> depth: 7 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_8.png" > <figcaption> depth: 8 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_9.png" > <figcaption> depth: 9 </figcaption> </figure>
        <figure> <img src="pt2.4/mcu/avg_contributions_10.png" > <figcaption> depth: 10 </figcaption> </figure>
    
        <h5> Cummulative Blend: </h5>
        <figure> <img src="pt2.4/mcu/cumulative_blend_strip.png" > <figcaption> Blending over time </figcaption> </figure>

<h4> Eh-oh! </h4>
        <h5> Teletubby Sun and a Labubu </h5>

        <div class="image-row">
            <figure> <img src="pt2.4/mcu/ironman.png"> <figcaption> Iron Man </figcaption> </figure>
            <figure> <img src="pt2.4/mcu/final.png"> <figcaption> Blend (regular straight mask)  </figcaption> </figure>
            <figure> <img src="pt2.4/mcu/tony.png"> <figcaption> Tony Stark </figcaption> </figure>

        </div>

        <h5> How? Depths = 9, sigma 0.5 </h5>
        
        <figure> <img src="pt2.4/horror/avg_contributions_0.png" > <figcaption> depth: 0 </figcaption> </figure>
        <figure> <img src="pt2.4/horror/avg_contributions_1.png" > <figcaption> depth: 1 </figcaption> </figure>
        <figure> <img src="pt2.4/horror/avg_contributions_2.png" > <figcaption> depth: 2 </figcaption> </figure>
        <figure> <img src="pt2.4/horror/avg_contributions_3.png" > <figcaption> depth: 3 </figcaption> </figure>
        <figure> <img src="pt2.4/horror/avg_contributions_4.png" > <figcaption> depth: 4 </figcaption> </figure>
        <figure> <img src="pt2.4/horror/avg_contributions_5.png" > <figcaption> depth: 5 </figcaption> </figure>
        <figure> <img src="pt2.4/horror/avg_contributions_6.png" > <figcaption> depth: 6 </figcaption> </figure>
        <figure> <img src="pt2.4/horror/avg_contributions_7.png" > <figcaption> depth: 7 </figcaption> </figure>
        <figure> <img src="pt2.4/horror/avg_contributions_8.png" > <figcaption> depth: 8 </figcaption> </figure>

    
        <h5> Cummulative Blend: </h5>
        <figure> <img src="pt2.4/horror/cumulative_blend_strip.png" > <figcaption> Blending over time </figcaption> </figure>
























</body>




</html>